{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CPU&GPU in CatBoost.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "02rEwnBojZHg",
        "-yWJinsGGg4p",
        "I1URbe3Ua6AZ",
        "3OVF4s1vm9G5",
        "jATbZJISmusl",
        "tdRJqSPSm0E_"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EudoraHan/CatBoost-Performance-Testing/blob/develop/CPU%26GPU_in_CatBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKk0UZUPF0PX",
        "colab_type": "text"
      },
      "source": [
        "# CatBoost Property Test\n",
        "\n",
        "### Yun Han 7/30/2019"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02rEwnBojZHg",
        "colab_type": "text"
      },
      "source": [
        "# 1. Epilson"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzfDELDOpk8M",
        "colab_type": "code",
        "outputId": "db12917f-d8d2-4f44-cf54-53bf57b27a7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "source": [
        "!pip install catboost"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.6/dist-packages (0.16.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from catboost) (3.0.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.12.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from catboost) (3.6.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: pandas>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from catboost) (0.24.2)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.16.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.5.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (1.1.0)\n",
            "Requirement already satisfied: nbformat>=4.2 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (4.4.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (2018.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (2.21.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Requirement already satisfied: decorator>=4.0.6 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (4.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->catboost) (41.0.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2->plotly->catboost) (2.6.0)\n",
            "Requirement already satisfied: traitlets>=4.1 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2->plotly->catboost) (4.3.2)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2->plotly->catboost) (0.2.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2->plotly->catboost) (4.5.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->plotly->catboost) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->plotly->catboost) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->plotly->catboost) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->plotly->catboost) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_iL4D2Hpq_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from catboost.datasets import epsilon\n",
        "\n",
        "train, test = epsilon()\n",
        "\n",
        "X_train, y_train = train.iloc[:,1:], train[0]\n",
        "X_test, y_test = test.iloc[:,1:], test[0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkI1UB-Vdmn5",
        "colab_type": "code",
        "outputId": "42e69f51-36f5-4d27-d1d0-5b0db38207cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400000, 2000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS1fuLxM9BNd",
        "colab_type": "code",
        "outputId": "72b4dfd7-02c2-48b5-99bd-96383c15a8e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100000, 2000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJjGXJqAPGID",
        "colab_type": "code",
        "outputId": "fdb154b7-7bc5-4ee7-8d3e-f34f8c18139a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!pip install -U -q PyDrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10kB 18.7MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 7.0MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 9.6MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 6.1MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 7.4MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 8.7MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71kB 9.9MB/s eta 0:00:01\r\u001b[K     |██▋                             | 81kB 11.0MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 12.1MB/s eta 0:00:01\r\u001b[K     |███▎                            | 102kB 9.6MB/s eta 0:00:01\r\u001b[K     |███▋                            | 112kB 9.6MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 9.6MB/s eta 0:00:01\r\u001b[K     |████▎                           | 133kB 9.6MB/s eta 0:00:01\r\u001b[K     |████▋                           | 143kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 153kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 163kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 174kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 184kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 194kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 204kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 215kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 225kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 235kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 245kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 256kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 266kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 286kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 296kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 317kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 327kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 348kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 358kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 378kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 389kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 409kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 419kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 440kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 450kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 460kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 471kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 481kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 491kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 501kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 512kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 522kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 532kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 542kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 552kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 563kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 573kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 583kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 593kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 604kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 614kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 624kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 634kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 645kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 655kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 665kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 675kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 686kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 696kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 706kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 716kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 727kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 737kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 747kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 757kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 768kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 778kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 788kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 798kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 808kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 819kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 829kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 839kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 849kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 860kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 870kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 880kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 890kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 901kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 911kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 921kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 931kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 942kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 952kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 962kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 972kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 983kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 993kB 9.6MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJihyDN9PZjz",
        "colab_type": "code",
        "outputId": "53d672ac-7e1c-4c74-8018-4e23614f3f25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "# Create & upload a text file.\n",
        "#你想要导出的文件的名字\n",
        "uploaded = drive.CreateFile({'title': 'OK.csv'})\n",
        "#改为之前生成文件的名字\n",
        "uploaded.SetContentFile('over.csv')\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded file with ID 1DPAcA9VzfP6HuhhSt0ifk9vXSsdCgobH\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CTmAlazPZmz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToDJ0LEzPZqB",
        "colab_type": "code",
        "outputId": "5c35f8b1-2748-452b-9e96-8b6fe62b6905",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_train.shape\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(400000, 2000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrR_I13os0N-",
        "colab_type": "text"
      },
      "source": [
        "### Training on CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVcWCSrnprCl",
        "colab_type": "code",
        "outputId": "a7e1c47b-5e45-4794-d719-3202b74b26a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "from catboost import CatBoostClassifier\n",
        "import timeit\n",
        "\n",
        "def train_on_cpu():  \n",
        "  model = CatBoostClassifier(\n",
        "    iterations=1000,\n",
        "    learning_rate=0.03\n",
        "  )\n",
        "  \n",
        "  model.fit(\n",
        "      X_train, y_train,\n",
        "      eval_set=(X_test, y_test),\n",
        "      verbose=10\n",
        "  );   \n",
        "      \n",
        "cpu_time = timeit.timeit('train_on_cpu()', \n",
        "                         setup=\"from __main__ import train_on_cpu\", \n",
        "                         number=1)\n",
        "\n",
        "print('Time to fit model on CPU: {} sec'.format(int(cpu_time)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0:\tlearn: 0.6878004\ttest: 0.6878645\tbest: 0.6878645 (0)\ttotal: 7.54s\tremaining: 2h 5m 33s\n",
            "10:\tlearn: 0.6459486\ttest: 0.6467108\tbest: 0.6467108 (10)\ttotal: 1m 24s\tremaining: 2h 6m 29s\n",
            "20:\tlearn: 0.6170315\ttest: 0.6180805\tbest: 0.6180805 (20)\ttotal: 2m 39s\tremaining: 2h 4m\n",
            "30:\tlearn: 0.5947894\ttest: 0.5960342\tbest: 0.5960342 (30)\ttotal: 3m 52s\tremaining: 2h 55s\n",
            "40:\tlearn: 0.5766997\ttest: 0.5780390\tbest: 0.5780390 (40)\ttotal: 5m 2s\tremaining: 1h 58m 5s\n",
            "50:\tlearn: 0.5616803\ttest: 0.5631138\tbest: 0.5631138 (50)\ttotal: 6m 10s\tremaining: 1h 54m 54s\n",
            "60:\tlearn: 0.5485252\ttest: 0.5499754\tbest: 0.5499754 (60)\ttotal: 7m 19s\tremaining: 1h 52m 42s\n",
            "70:\tlearn: 0.5371519\ttest: 0.5385685\tbest: 0.5385685 (70)\ttotal: 8m 29s\tremaining: 1h 51m 6s\n",
            "80:\tlearn: 0.5267357\ttest: 0.5282210\tbest: 0.5282210 (80)\ttotal: 9m 40s\tremaining: 1h 49m 48s\n",
            "90:\tlearn: 0.5175479\ttest: 0.5190800\tbest: 0.5190800 (90)\ttotal: 10m 52s\tremaining: 1h 48m 36s\n",
            "100:\tlearn: 0.5092512\ttest: 0.5108786\tbest: 0.5108786 (100)\ttotal: 12m 7s\tremaining: 1h 47m 51s\n",
            "110:\tlearn: 0.5014700\ttest: 0.5031395\tbest: 0.5031395 (110)\ttotal: 13m 12s\tremaining: 1h 45m 50s\n",
            "120:\tlearn: 0.4944835\ttest: 0.4961855\tbest: 0.4961855 (120)\ttotal: 14m 15s\tremaining: 1h 43m 36s\n",
            "130:\tlearn: 0.4878167\ttest: 0.4894882\tbest: 0.4894882 (130)\ttotal: 15m 19s\tremaining: 1h 41m 40s\n",
            "140:\tlearn: 0.4815179\ttest: 0.4833202\tbest: 0.4833202 (140)\ttotal: 16m 24s\tremaining: 1h 39m 56s\n",
            "150:\tlearn: 0.4758605\ttest: 0.4777254\tbest: 0.4777254 (150)\ttotal: 17m 27s\tremaining: 1h 38m 8s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMN0yoZEwIvt",
        "colab_type": "text"
      },
      "source": [
        "### Training on GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3FWmPi8prIZ",
        "colab_type": "code",
        "outputId": "441e5ba7-8df6-45b2-e422-b0a05829462a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "def train_on_gpu():  \n",
        "  model = CatBoostClassifier(\n",
        "    iterations=100,\n",
        "    learning_rate=0.03,\n",
        "    task_type='GPU'\n",
        "  )\n",
        "  \n",
        "  model.fit(\n",
        "      X_train, y_train,\n",
        "      eval_set=(X_test, y_test),\n",
        "      verbose=10\n",
        "  );     \n",
        "      \n",
        "gpu_time = timeit.timeit('train_on_gpu()', \n",
        "                         setup=\"from __main__ import train_on_gpu\", \n",
        "                         number=1)\n",
        "\n",
        "print('Time to fit model on GPU: {} sec'.format(int(gpu_time)))\n",
        "print('GPU speedup over CPU: ' + '%.2f' % (cpu_time/gpu_time) + 'x')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0:\tlearn: 0.6877672\ttest: 0.6878640\tbest: 0.6878640 (0)\ttotal: 204ms\tremaining: 20.2s\n",
            "10:\tlearn: 0.6457422\ttest: 0.6464576\tbest: 0.6464576 (10)\ttotal: 1.3s\tremaining: 10.6s\n",
            "20:\tlearn: 0.6163275\ttest: 0.6174585\tbest: 0.6174585 (20)\ttotal: 2.36s\tremaining: 8.87s\n",
            "30:\tlearn: 0.5943049\ttest: 0.5956978\tbest: 0.5956978 (30)\ttotal: 3.38s\tremaining: 7.52s\n",
            "40:\tlearn: 0.5763316\ttest: 0.5778145\tbest: 0.5778145 (40)\ttotal: 4.39s\tremaining: 6.32s\n",
            "50:\tlearn: 0.5607705\ttest: 0.5623940\tbest: 0.5623940 (50)\ttotal: 5.42s\tremaining: 5.2s\n",
            "60:\tlearn: 0.5478194\ttest: 0.5495075\tbest: 0.5495075 (60)\ttotal: 6.41s\tremaining: 4.09s\n",
            "70:\tlearn: 0.5360011\ttest: 0.5377425\tbest: 0.5377425 (70)\ttotal: 7.37s\tremaining: 3.01s\n",
            "80:\tlearn: 0.5258042\ttest: 0.5275755\tbest: 0.5275755 (80)\ttotal: 8.37s\tremaining: 1.96s\n",
            "90:\tlearn: 0.5165436\ttest: 0.5183847\tbest: 0.5183847 (90)\ttotal: 9.38s\tremaining: 927ms\n",
            "99:\tlearn: 0.5089647\ttest: 0.5108700\tbest: 0.5108700 (99)\ttotal: 10.2s\tremaining: 0us\n",
            "bestTest = 0.5108699609\n",
            "bestIteration = 99\n",
            "Shrink model to first 100 iterations.\n",
            "Time to fit model on GPU: 124 sec\n",
            "GPU speedup over CPU: 6.46x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOFCqGvVxuSg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from catboost import CatBoostClassifier\n",
        "classifier_cat = CatBoostClassifier(iterations = 100, task_type = 'GPU')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxhXIPouxuaU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model\n",
        "classifier_cat.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3HwyLAExudd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prediction of test data\n",
        "classifier_cat.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LU830QZx02D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Accuracy of test data\n",
        "classifier_cat.score(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yWJinsGGg4p",
        "colab_type": "text"
      },
      "source": [
        "# 2. Credit card lost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxamOecuGjiS",
        "colab_type": "code",
        "outputId": "7990b2c1-fe0f-47e5-a4b6-10b8dfeb7811",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d3fce673-ef9d-4eef-ab62-2bf2b4dfbb62\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-d3fce673-ef9d-4eef-ab62-2bf2b4dfbb62\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving creditcard lost.csv to creditcard lost.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlLXTnREGjsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(io.BytesIO(uploaded['creditcard lost.csv']))\n",
        "#df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CibITPH3Jqqu",
        "colab_type": "code",
        "outputId": "30fc3890-03ab-46e4-ea25-00c990fa1a4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>y</th>\n",
              "      <th>x1</th>\n",
              "      <th>x2</th>\n",
              "      <th>x3</th>\n",
              "      <th>x4</th>\n",
              "      <th>x5</th>\n",
              "      <th>x6</th>\n",
              "      <th>x7</th>\n",
              "      <th>x8</th>\n",
              "      <th>x9</th>\n",
              "      <th>x10</th>\n",
              "      <th>x11</th>\n",
              "      <th>x12</th>\n",
              "      <th>x13</th>\n",
              "      <th>x14</th>\n",
              "      <th>x15</th>\n",
              "      <th>x16</th>\n",
              "      <th>x17</th>\n",
              "      <th>x18</th>\n",
              "      <th>x19</th>\n",
              "      <th>x20</th>\n",
              "      <th>x21</th>\n",
              "      <th>x22</th>\n",
              "      <th>x23</th>\n",
              "      <th>x24</th>\n",
              "      <th>x25</th>\n",
              "      <th>x26</th>\n",
              "      <th>x27</th>\n",
              "      <th>x28</th>\n",
              "      <th>x29</th>\n",
              "      <th>x30</th>\n",
              "      <th>x31</th>\n",
              "      <th>x32</th>\n",
              "      <th>x33</th>\n",
              "      <th>x34</th>\n",
              "      <th>x35</th>\n",
              "      <th>x36</th>\n",
              "      <th>x37</th>\n",
              "      <th>x38</th>\n",
              "      <th>x39</th>\n",
              "      <th>...</th>\n",
              "      <th>x81</th>\n",
              "      <th>x82</th>\n",
              "      <th>x83</th>\n",
              "      <th>x84</th>\n",
              "      <th>x85</th>\n",
              "      <th>x86</th>\n",
              "      <th>x87</th>\n",
              "      <th>x88</th>\n",
              "      <th>x89</th>\n",
              "      <th>x90</th>\n",
              "      <th>x91</th>\n",
              "      <th>x92</th>\n",
              "      <th>x93</th>\n",
              "      <th>x94</th>\n",
              "      <th>x95</th>\n",
              "      <th>x96</th>\n",
              "      <th>x97</th>\n",
              "      <th>x98</th>\n",
              "      <th>x99</th>\n",
              "      <th>x100</th>\n",
              "      <th>x101</th>\n",
              "      <th>x102</th>\n",
              "      <th>x103</th>\n",
              "      <th>x104</th>\n",
              "      <th>x105</th>\n",
              "      <th>x106</th>\n",
              "      <th>x107</th>\n",
              "      <th>x108</th>\n",
              "      <th>x109</th>\n",
              "      <th>x110</th>\n",
              "      <th>x111</th>\n",
              "      <th>x112</th>\n",
              "      <th>x113</th>\n",
              "      <th>x114</th>\n",
              "      <th>x115</th>\n",
              "      <th>x116</th>\n",
              "      <th>x117</th>\n",
              "      <th>x118</th>\n",
              "      <th>x119</th>\n",
              "      <th>x120</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>5.04</td>\n",
              "      <td>5.77</td>\n",
              "      <td>6.04</td>\n",
              "      <td>3.91</td>\n",
              "      <td>0.66</td>\n",
              "      <td>1.04</td>\n",
              "      <td>1.83</td>\n",
              "      <td>5.41</td>\n",
              "      <td>5.45</td>\n",
              "      <td>6.91</td>\n",
              "      <td>6.70</td>\n",
              "      <td>1.17</td>\n",
              "      <td>3.08</td>\n",
              "      <td>3.44</td>\n",
              "      <td>1.83</td>\n",
              "      <td>5.32</td>\n",
              "      <td>5.16</td>\n",
              "      <td>4.62</td>\n",
              "      <td>4.58</td>\n",
              "      <td>3.98</td>\n",
              "      <td>6.57</td>\n",
              "      <td>5.18</td>\n",
              "      <td>1.79</td>\n",
              "      <td>2.99</td>\n",
              "      <td>7.95</td>\n",
              "      <td>21.02</td>\n",
              "      <td>1.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4.55</td>\n",
              "      <td>6.90</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.25</td>\n",
              "      <td>3.16</td>\n",
              "      <td>5.85</td>\n",
              "      <td>6.26</td>\n",
              "      <td>5.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>4.94</td>\n",
              "      <td>5.75</td>\n",
              "      <td>6.23</td>\n",
              "      <td>5.80</td>\n",
              "      <td>1.45</td>\n",
              "      <td>2.63</td>\n",
              "      <td>2.19</td>\n",
              "      <td>5.18</td>\n",
              "      <td>8.17</td>\n",
              "      <td>3.39</td>\n",
              "      <td>1.14</td>\n",
              "      <td>7.95</td>\n",
              "      <td>7.95</td>\n",
              "      <td>8.52</td>\n",
              "      <td>3.41</td>\n",
              "      <td>3.98</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.45</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.45</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>20.69</td>\n",
              "      <td>7.94</td>\n",
              "      <td>7.70</td>\n",
              "      <td>2.97</td>\n",
              "      <td>6.88</td>\n",
              "      <td>4.54</td>\n",
              "      <td>0.68</td>\n",
              "      <td>7.31</td>\n",
              "      <td>8.04</td>\n",
              "      <td>10.73</td>\n",
              "      <td>5.58</td>\n",
              "      <td>0.00</td>\n",
              "      <td>7.52</td>\n",
              "      <td>5.37</td>\n",
              "      <td>6.18</td>\n",
              "      <td>4.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>5.98</td>\n",
              "      <td>2.90</td>\n",
              "      <td>1.94</td>\n",
              "      <td>1.42</td>\n",
              "      <td>0.10</td>\n",
              "      <td>3.29</td>\n",
              "      <td>0.81</td>\n",
              "      <td>9.09</td>\n",
              "      <td>8.04</td>\n",
              "      <td>4.65</td>\n",
              "      <td>2.65</td>\n",
              "      <td>0.46</td>\n",
              "      <td>2.25</td>\n",
              "      <td>6.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>6.19</td>\n",
              "      <td>5.06</td>\n",
              "      <td>0.56</td>\n",
              "      <td>4.80</td>\n",
              "      <td>6.55</td>\n",
              "      <td>3.49</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.93</td>\n",
              "      <td>10.62</td>\n",
              "      <td>4.28</td>\n",
              "      <td>3.83</td>\n",
              "      <td>3.98</td>\n",
              "      <td>7.52</td>\n",
              "      <td>24.07</td>\n",
              "      <td>9.26</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.04</td>\n",
              "      <td>10.31</td>\n",
              "      <td>9.35</td>\n",
              "      <td>4.45</td>\n",
              "      <td>0.04</td>\n",
              "      <td>...</td>\n",
              "      <td>5.17</td>\n",
              "      <td>7.54</td>\n",
              "      <td>13.27</td>\n",
              "      <td>3.82</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.34</td>\n",
              "      <td>0.00</td>\n",
              "      <td>10.48</td>\n",
              "      <td>6.55</td>\n",
              "      <td>0.44</td>\n",
              "      <td>0.00</td>\n",
              "      <td>12.83</td>\n",
              "      <td>12.83</td>\n",
              "      <td>5.31</td>\n",
              "      <td>2.51</td>\n",
              "      <td>1.33</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.63</td>\n",
              "      <td>1.85</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.78</td>\n",
              "      <td>2.78</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.93</td>\n",
              "      <td>12.21</td>\n",
              "      <td>6.96</td>\n",
              "      <td>0.10</td>\n",
              "      <td>4.20</td>\n",
              "      <td>0.88</td>\n",
              "      <td>0.19</td>\n",
              "      <td>10.44</td>\n",
              "      <td>7.17</td>\n",
              "      <td>2.96</td>\n",
              "      <td>6.75</td>\n",
              "      <td>0.17</td>\n",
              "      <td>5.40</td>\n",
              "      <td>8.32</td>\n",
              "      <td>5.40</td>\n",
              "      <td>1.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>7.04</td>\n",
              "      <td>8.92</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4.50</td>\n",
              "      <td>0.00</td>\n",
              "      <td>8.41</td>\n",
              "      <td>6.97</td>\n",
              "      <td>12.65</td>\n",
              "      <td>9.24</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.54</td>\n",
              "      <td>10.11</td>\n",
              "      <td>0.06</td>\n",
              "      <td>10.43</td>\n",
              "      <td>12.13</td>\n",
              "      <td>1.42</td>\n",
              "      <td>13.91</td>\n",
              "      <td>10.66</td>\n",
              "      <td>3.73</td>\n",
              "      <td>3.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.61</td>\n",
              "      <td>2.86</td>\n",
              "      <td>8.57</td>\n",
              "      <td>10.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>8.57</td>\n",
              "      <td>11.11</td>\n",
              "      <td>3.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.70</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>9.51</td>\n",
              "      <td>5.19</td>\n",
              "      <td>6.20</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>7.11</td>\n",
              "      <td>8.28</td>\n",
              "      <td>8.66</td>\n",
              "      <td>5.33</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.00</td>\n",
              "      <td>6.01</td>\n",
              "      <td>6.16</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.00</td>\n",
              "      <td>10.00</td>\n",
              "      <td>7.14</td>\n",
              "      <td>11.43</td>\n",
              "      <td>20.00</td>\n",
              "      <td>2.86</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.41</td>\n",
              "      <td>14.81</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>11.11</td>\n",
              "      <td>8.47</td>\n",
              "      <td>9.45</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4.60</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.77</td>\n",
              "      <td>2.70</td>\n",
              "      <td>6.84</td>\n",
              "      <td>7.28</td>\n",
              "      <td>0.00</td>\n",
              "      <td>5.60</td>\n",
              "      <td>7.52</td>\n",
              "      <td>5.93</td>\n",
              "      <td>3.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>13.30</td>\n",
              "      <td>7.16</td>\n",
              "      <td>5.69</td>\n",
              "      <td>1.98</td>\n",
              "      <td>0.71</td>\n",
              "      <td>2.20</td>\n",
              "      <td>0.22</td>\n",
              "      <td>13.89</td>\n",
              "      <td>1.75</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.27</td>\n",
              "      <td>0.00</td>\n",
              "      <td>10.26</td>\n",
              "      <td>9.46</td>\n",
              "      <td>4.42</td>\n",
              "      <td>4.14</td>\n",
              "      <td>4.08</td>\n",
              "      <td>4.32</td>\n",
              "      <td>4.22</td>\n",
              "      <td>4.11</td>\n",
              "      <td>3.83</td>\n",
              "      <td>4.08</td>\n",
              "      <td>4.36</td>\n",
              "      <td>3.99</td>\n",
              "      <td>7.96</td>\n",
              "      <td>6.19</td>\n",
              "      <td>5.31</td>\n",
              "      <td>4.42</td>\n",
              "      <td>11.50</td>\n",
              "      <td>32.14</td>\n",
              "      <td>3.57</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.13</td>\n",
              "      <td>5.83</td>\n",
              "      <td>1.45</td>\n",
              "      <td>11.60</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>4.11</td>\n",
              "      <td>4.15</td>\n",
              "      <td>4.13</td>\n",
              "      <td>4.20</td>\n",
              "      <td>4.37</td>\n",
              "      <td>4.30</td>\n",
              "      <td>4.54</td>\n",
              "      <td>4.15</td>\n",
              "      <td>4.11</td>\n",
              "      <td>4.28</td>\n",
              "      <td>0.00</td>\n",
              "      <td>9.73</td>\n",
              "      <td>4.42</td>\n",
              "      <td>3.54</td>\n",
              "      <td>7.96</td>\n",
              "      <td>4.42</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.14</td>\n",
              "      <td>3.57</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.57</td>\n",
              "      <td>2.95</td>\n",
              "      <td>7.28</td>\n",
              "      <td>0.00</td>\n",
              "      <td>9.54</td>\n",
              "      <td>5.71</td>\n",
              "      <td>0.48</td>\n",
              "      <td>4.77</td>\n",
              "      <td>1.13</td>\n",
              "      <td>2.67</td>\n",
              "      <td>8.83</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4.09</td>\n",
              "      <td>4.14</td>\n",
              "      <td>4.16</td>\n",
              "      <td>4.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>5.24</td>\n",
              "      <td>5.09</td>\n",
              "      <td>5.31</td>\n",
              "      <td>1.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4.24</td>\n",
              "      <td>0.34</td>\n",
              "      <td>5.25</td>\n",
              "      <td>5.59</td>\n",
              "      <td>9.58</td>\n",
              "      <td>3.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>5.85</td>\n",
              "      <td>10.89</td>\n",
              "      <td>2.16</td>\n",
              "      <td>6.13</td>\n",
              "      <td>5.88</td>\n",
              "      <td>4.22</td>\n",
              "      <td>5.15</td>\n",
              "      <td>5.54</td>\n",
              "      <td>5.85</td>\n",
              "      <td>4.68</td>\n",
              "      <td>1.15</td>\n",
              "      <td>1.54</td>\n",
              "      <td>9.42</td>\n",
              "      <td>7.48</td>\n",
              "      <td>2.77</td>\n",
              "      <td>3.88</td>\n",
              "      <td>6.65</td>\n",
              "      <td>13.64</td>\n",
              "      <td>3.03</td>\n",
              "      <td>4.55</td>\n",
              "      <td>4.55</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.08</td>\n",
              "      <td>8.67</td>\n",
              "      <td>8.11</td>\n",
              "      <td>4.08</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>5.93</td>\n",
              "      <td>6.01</td>\n",
              "      <td>6.05</td>\n",
              "      <td>6.07</td>\n",
              "      <td>1.64</td>\n",
              "      <td>3.17</td>\n",
              "      <td>2.25</td>\n",
              "      <td>6.44</td>\n",
              "      <td>6.60</td>\n",
              "      <td>2.91</td>\n",
              "      <td>0.28</td>\n",
              "      <td>9.14</td>\n",
              "      <td>9.42</td>\n",
              "      <td>4.99</td>\n",
              "      <td>3.60</td>\n",
              "      <td>2.49</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.15</td>\n",
              "      <td>1.52</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.55</td>\n",
              "      <td>9.09</td>\n",
              "      <td>7.24</td>\n",
              "      <td>7.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>5.11</td>\n",
              "      <td>3.71</td>\n",
              "      <td>0.00</td>\n",
              "      <td>6.01</td>\n",
              "      <td>4.16</td>\n",
              "      <td>4.49</td>\n",
              "      <td>6.14</td>\n",
              "      <td>0.00</td>\n",
              "      <td>6.18</td>\n",
              "      <td>6.10</td>\n",
              "      <td>6.06</td>\n",
              "      <td>5.86</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 121 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   y     x1    x2    x3    x4    x5  ...  x115  x116  x117  x118  x119  x120\n",
              "0  1   5.04  5.77  6.04  3.91  0.66  ...  5.58  0.00  7.52  5.37  6.18  4.24\n",
              "1  1   5.98  2.90  1.94  1.42  0.10  ...  6.75  0.17  5.40  8.32  5.40  1.57\n",
              "2  1   7.04  8.92  3.24  0.01  0.00  ...  7.28  0.00  5.60  7.52  5.93  3.89\n",
              "3  1  13.30  7.16  5.69  1.98  0.71  ...  8.83  0.00  4.09  4.14  4.16  4.23\n",
              "4  1   5.24  5.09  5.31  1.05  0.00  ...  6.14  0.00  6.18  6.10  6.06  5.86\n",
              "\n",
              "[5 rows x 121 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpm0ueCvJqxF",
        "colab_type": "code",
        "outputId": "e1adb47f-a7ed-4757-de86-d4926f73abf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "X"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.04, 5.77, 6.04, ..., 5.37, 6.18, 4.24],\n",
              "       [5.98, 2.9 , 1.94, ..., 8.32, 5.4 , 1.57],\n",
              "       [7.04, 8.92, 3.24, ..., 7.52, 5.93, 3.89],\n",
              "       ...,\n",
              "       [2.74, 5.42, 8.79, ..., 6.75, 4.98, 5.21],\n",
              "       [7.51, 6.76, 4.73, ..., 4.94, 5.  , 4.93],\n",
              "       [3.81, 6.79, 7.66, ..., 5.17, 5.06, 4.72]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHWE74ptGjww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get X and y\n",
        "X = df.iloc[:, 1:].values\n",
        "y = df.iloc[:, 0].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYSvW2aaIFUZ",
        "colab_type": "code",
        "outputId": "565cf412-b082-4f26-b7d1-07741760c6a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Split data into training and testing\n",
        "from sklearn import model_selection\n",
        "\n",
        "# Reserve 20% for testing\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.2)\n",
        "\n",
        "print('Training data has %d observation with %d features' % X_train.shape)\n",
        "print('Test data has %d observation with %d features' % X_test.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data has 12000 observation with 120 features\n",
            "Test data has 3000 observation with 120 features\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCZN4B_PIa5T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "4a1fe4db-66ee-4e96-df3b-fb6296a980c3"
      },
      "source": [
        "!pip install catboost\n",
        "from catboost import CatBoostClassifier\n",
        "classifier_cat = CatBoostClassifier(iterations = 100, task_type = 'GPU')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.6/dist-packages (0.16.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from catboost) (3.0.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.6/dist-packages (from catboost) (3.6.1)\n",
            "Requirement already satisfied: pandas>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from catboost) (0.24.2)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.6/dist-packages (from catboost) (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from catboost) (1.16.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from catboost) (1.12.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.5.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (2.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->catboost) (1.1.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (1.3.3)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (2018.9)\n",
            "Requirement already satisfied: decorator>=4.0.6 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (4.4.0)\n",
            "Requirement already satisfied: nbformat>=4.2 in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (4.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from plotly->catboost) (2.21.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->catboost) (41.0.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2->plotly->catboost) (2.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2->plotly->catboost) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.1 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2->plotly->catboost) (4.3.2)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2->plotly->catboost) (4.5.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->plotly->catboost) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->plotly->catboost) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->plotly->catboost) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->plotly->catboost) (2019.6.16)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGnAgM4wEK5S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e01905bf-d205-4232-a2fd-410764dfb730"
      },
      "source": [
        "X_train.shape\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12000, 120)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls-xjgz7Idx4",
        "colab_type": "code",
        "outputId": "8345b22e-5d5a-4c34-ca9f-88e6cb6ba206",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "import timeit\n",
        "def train_on_cpu():  \n",
        "  model = CatBoostClassifier(\n",
        "    iterations=1000,\n",
        "    learning_rate=0.03\n",
        "  )\n",
        "  \n",
        "  model.fit(\n",
        "      X_train, y_train,\n",
        "      eval_set=(X_test, y_test),\n",
        "      verbose=100\n",
        "  );   \n",
        "      \n",
        "cpu_time = timeit.timeit('train_on_cpu()', \n",
        "                         setup=\"from __main__ import train_on_cpu\", \n",
        "                         number=1)\n",
        "\n",
        "print('Time to fit model on CPU: {} sec'.format(int(cpu_time)))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0:\tlearn: 0.6678724\ttest: 0.6675403\tbest: 0.6675403 (0)\ttotal: 264ms\tremaining: 4m 24s\n",
            "100:\tlearn: 0.3198527\ttest: 0.3176252\tbest: 0.3176252 (100)\ttotal: 26.4s\tremaining: 3m 54s\n",
            "200:\tlearn: 0.3068348\ttest: 0.3165573\tbest: 0.3164839 (196)\ttotal: 51.4s\tremaining: 3m 24s\n",
            "300:\tlearn: 0.2974008\ttest: 0.3169241\tbest: 0.3163827 (208)\ttotal: 1m 15s\tremaining: 2m 56s\n",
            "400:\tlearn: 0.2874031\ttest: 0.3172878\tbest: 0.3163827 (208)\ttotal: 1m 40s\tremaining: 2m 29s\n",
            "500:\tlearn: 0.2786824\ttest: 0.3177666\tbest: 0.3163827 (208)\ttotal: 2m 4s\tremaining: 2m 3s\n",
            "600:\tlearn: 0.2700714\ttest: 0.3180958\tbest: 0.3163827 (208)\ttotal: 2m 28s\tremaining: 1m 38s\n",
            "700:\tlearn: 0.2630478\ttest: 0.3188492\tbest: 0.3163827 (208)\ttotal: 2m 52s\tremaining: 1m 13s\n",
            "800:\tlearn: 0.2564199\ttest: 0.3192582\tbest: 0.3163827 (208)\ttotal: 3m 15s\tremaining: 48.7s\n",
            "900:\tlearn: 0.2508234\ttest: 0.3196992\tbest: 0.3163827 (208)\ttotal: 3m 39s\tremaining: 24.1s\n",
            "999:\tlearn: 0.2447655\ttest: 0.3200633\tbest: 0.3163827 (208)\ttotal: 4m 3s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.3163827465\n",
            "bestIteration = 208\n",
            "\n",
            "Shrink model to first 209 iterations.\n",
            "Time to fit model on CPU: 243 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLWaa9eNQ0DZ",
        "colab_type": "code",
        "outputId": "f460b9a6-e07d-4d5c-c15b-d5f673af454c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "def train_on_gpu():  \n",
        "  model = CatBoostClassifier(\n",
        "    iterations=1000,\n",
        "    learning_rate=0.03,\n",
        "    task_type='GPU'\n",
        "  )\n",
        "  \n",
        "  model.fit(\n",
        "      X_train, y_train,\n",
        "      eval_set=(X_test, y_test),\n",
        "      verbose=100\n",
        "  );     \n",
        "      \n",
        "gpu_time = timeit.timeit('train_on_gpu()', \n",
        "                         setup=\"from __main__ import train_on_gpu\", \n",
        "                         number=1)\n",
        "\n",
        "print('Time to fit model on GPU: {} sec'.format(int(gpu_time)))\n",
        "print('GPU speedup over CPU: ' + '%.2f' % (cpu_time/gpu_time) + 'x')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0:\tlearn: 0.6678815\ttest: 0.6674659\tbest: 0.6674659 (0)\ttotal: 75.3ms\tremaining: 1m 15s\n",
            "100:\tlearn: 0.3164914\ttest: 0.3169017\tbest: 0.3169017 (100)\ttotal: 4.13s\tremaining: 36.8s\n",
            "200:\tlearn: 0.3025696\ttest: 0.3158918\tbest: 0.3157258 (180)\ttotal: 8.06s\tremaining: 32s\n",
            "300:\tlearn: 0.2915746\ttest: 0.3155904\tbest: 0.3155712 (299)\ttotal: 12s\tremaining: 27.8s\n",
            "400:\tlearn: 0.2813490\ttest: 0.3159931\tbest: 0.3155262 (377)\ttotal: 15.9s\tremaining: 23.8s\n",
            "500:\tlearn: 0.2709261\ttest: 0.3169734\tbest: 0.3155262 (377)\ttotal: 19.9s\tremaining: 19.8s\n",
            "600:\tlearn: 0.2622394\ttest: 0.3172880\tbest: 0.3155262 (377)\ttotal: 23.8s\tremaining: 15.8s\n",
            "700:\tlearn: 0.2531303\ttest: 0.3174485\tbest: 0.3155262 (377)\ttotal: 27.8s\tremaining: 11.8s\n",
            "800:\tlearn: 0.2452772\ttest: 0.3179960\tbest: 0.3155262 (377)\ttotal: 31.8s\tremaining: 7.89s\n",
            "900:\tlearn: 0.2378677\ttest: 0.3185316\tbest: 0.3155262 (377)\ttotal: 35.7s\tremaining: 3.92s\n",
            "999:\tlearn: 0.2316635\ttest: 0.3188425\tbest: 0.3155262 (377)\ttotal: 39.7s\tremaining: 0us\n",
            "bestTest = 0.3155262044\n",
            "bestIteration = 377\n",
            "Shrink model to first 378 iterations.\n",
            "Time to fit model on GPU: 40 sec\n",
            "GPU speedup over CPU: 6.04x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7uTnCJpTVuI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKyAA06ETV1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import lightgbm as lgb\n",
        "lgb_train = lgb.Dataset(X_train, y_train)\n",
        "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
        "# specify your configurations as a dict\n",
        "params = {\n",
        "    'task': 'train',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'binary',\n",
        "    'metric': {'binary_logloss'},\n",
        "    'learning_rate': 0.03\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WHhKh-bTWXA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "edc3c561-4f36-42cb-a53d-0546c770f807"
      },
      "source": [
        "# Use time function to measure time elapsed\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "\n",
        "gbm = lgb.train(params,\n",
        "                lgb_train,\n",
        "                num_boost_round=1000,\n",
        "                valid_sets=lgb_train)\n",
        "\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1]\ttraining's binary_logloss: 0.332892\n",
            "[2]\ttraining's binary_logloss: 0.331325\n",
            "[3]\ttraining's binary_logloss: 0.329847\n",
            "[4]\ttraining's binary_logloss: 0.328469\n",
            "[5]\ttraining's binary_logloss: 0.326981\n",
            "[6]\ttraining's binary_logloss: 0.325567\n",
            "[7]\ttraining's binary_logloss: 0.324211\n",
            "[8]\ttraining's binary_logloss: 0.32296\n",
            "[9]\ttraining's binary_logloss: 0.321699\n",
            "[10]\ttraining's binary_logloss: 0.320385\n",
            "[11]\ttraining's binary_logloss: 0.319137\n",
            "[12]\ttraining's binary_logloss: 0.317881\n",
            "[13]\ttraining's binary_logloss: 0.316514\n",
            "[14]\ttraining's binary_logloss: 0.315302\n",
            "[15]\ttraining's binary_logloss: 0.314045\n",
            "[16]\ttraining's binary_logloss: 0.312827\n",
            "[17]\ttraining's binary_logloss: 0.3116\n",
            "[18]\ttraining's binary_logloss: 0.310397\n",
            "[19]\ttraining's binary_logloss: 0.309276\n",
            "[20]\ttraining's binary_logloss: 0.308279\n",
            "[21]\ttraining's binary_logloss: 0.30713\n",
            "[22]\ttraining's binary_logloss: 0.30598\n",
            "[23]\ttraining's binary_logloss: 0.30497\n",
            "[24]\ttraining's binary_logloss: 0.303916\n",
            "[25]\ttraining's binary_logloss: 0.302786\n",
            "[26]\ttraining's binary_logloss: 0.301741\n",
            "[27]\ttraining's binary_logloss: 0.30066\n",
            "[28]\ttraining's binary_logloss: 0.299695\n",
            "[29]\ttraining's binary_logloss: 0.298658\n",
            "[30]\ttraining's binary_logloss: 0.297543\n",
            "[31]\ttraining's binary_logloss: 0.29655\n",
            "[32]\ttraining's binary_logloss: 0.295528\n",
            "[33]\ttraining's binary_logloss: 0.294507\n",
            "[34]\ttraining's binary_logloss: 0.293401\n",
            "[35]\ttraining's binary_logloss: 0.292414\n",
            "[36]\ttraining's binary_logloss: 0.291541\n",
            "[37]\ttraining's binary_logloss: 0.290622\n",
            "[38]\ttraining's binary_logloss: 0.289556\n",
            "[39]\ttraining's binary_logloss: 0.288491\n",
            "[40]\ttraining's binary_logloss: 0.287518\n",
            "[41]\ttraining's binary_logloss: 0.286605\n",
            "[42]\ttraining's binary_logloss: 0.285697\n",
            "[43]\ttraining's binary_logloss: 0.284816\n",
            "[44]\ttraining's binary_logloss: 0.283998\n",
            "[45]\ttraining's binary_logloss: 0.283142\n",
            "[46]\ttraining's binary_logloss: 0.282295\n",
            "[47]\ttraining's binary_logloss: 0.281366\n",
            "[48]\ttraining's binary_logloss: 0.280523\n",
            "[49]\ttraining's binary_logloss: 0.279729\n",
            "[50]\ttraining's binary_logloss: 0.27886\n",
            "[51]\ttraining's binary_logloss: 0.277923\n",
            "[52]\ttraining's binary_logloss: 0.277183\n",
            "[53]\ttraining's binary_logloss: 0.276258\n",
            "[54]\ttraining's binary_logloss: 0.275403\n",
            "[55]\ttraining's binary_logloss: 0.274544\n",
            "[56]\ttraining's binary_logloss: 0.273673\n",
            "[57]\ttraining's binary_logloss: 0.272887\n",
            "[58]\ttraining's binary_logloss: 0.272019\n",
            "[59]\ttraining's binary_logloss: 0.271303\n",
            "[60]\ttraining's binary_logloss: 0.270553\n",
            "[61]\ttraining's binary_logloss: 0.269743\n",
            "[62]\ttraining's binary_logloss: 0.268889\n",
            "[63]\ttraining's binary_logloss: 0.268202\n",
            "[64]\ttraining's binary_logloss: 0.267291\n",
            "[65]\ttraining's binary_logloss: 0.266464\n",
            "[66]\ttraining's binary_logloss: 0.265649\n",
            "[67]\ttraining's binary_logloss: 0.26482\n",
            "[68]\ttraining's binary_logloss: 0.263967\n",
            "[69]\ttraining's binary_logloss: 0.263285\n",
            "[70]\ttraining's binary_logloss: 0.262619\n",
            "[71]\ttraining's binary_logloss: 0.261837\n",
            "[72]\ttraining's binary_logloss: 0.261163\n",
            "[73]\ttraining's binary_logloss: 0.260398\n",
            "[74]\ttraining's binary_logloss: 0.259618\n",
            "[75]\ttraining's binary_logloss: 0.258995\n",
            "[76]\ttraining's binary_logloss: 0.25828\n",
            "[77]\ttraining's binary_logloss: 0.257633\n",
            "[78]\ttraining's binary_logloss: 0.257029\n",
            "[79]\ttraining's binary_logloss: 0.256334\n",
            "[80]\ttraining's binary_logloss: 0.255558\n",
            "[81]\ttraining's binary_logloss: 0.254816\n",
            "[82]\ttraining's binary_logloss: 0.254021\n",
            "[83]\ttraining's binary_logloss: 0.253329\n",
            "[84]\ttraining's binary_logloss: 0.252594\n",
            "[85]\ttraining's binary_logloss: 0.251965\n",
            "[86]\ttraining's binary_logloss: 0.251323\n",
            "[87]\ttraining's binary_logloss: 0.250589\n",
            "[88]\ttraining's binary_logloss: 0.249879\n",
            "[89]\ttraining's binary_logloss: 0.249075\n",
            "[90]\ttraining's binary_logloss: 0.248472\n",
            "[91]\ttraining's binary_logloss: 0.247783\n",
            "[92]\ttraining's binary_logloss: 0.247055\n",
            "[93]\ttraining's binary_logloss: 0.246379\n",
            "[94]\ttraining's binary_logloss: 0.245733\n",
            "[95]\ttraining's binary_logloss: 0.245098\n",
            "[96]\ttraining's binary_logloss: 0.244452\n",
            "[97]\ttraining's binary_logloss: 0.243814\n",
            "[98]\ttraining's binary_logloss: 0.243116\n",
            "[99]\ttraining's binary_logloss: 0.242538\n",
            "[100]\ttraining's binary_logloss: 0.24187\n",
            "[101]\ttraining's binary_logloss: 0.241199\n",
            "[102]\ttraining's binary_logloss: 0.240577\n",
            "[103]\ttraining's binary_logloss: 0.23999\n",
            "[104]\ttraining's binary_logloss: 0.239404\n",
            "[105]\ttraining's binary_logloss: 0.238708\n",
            "[106]\ttraining's binary_logloss: 0.238064\n",
            "[107]\ttraining's binary_logloss: 0.237324\n",
            "[108]\ttraining's binary_logloss: 0.236784\n",
            "[109]\ttraining's binary_logloss: 0.236138\n",
            "[110]\ttraining's binary_logloss: 0.235619\n",
            "[111]\ttraining's binary_logloss: 0.234996\n",
            "[112]\ttraining's binary_logloss: 0.23439\n",
            "[113]\ttraining's binary_logloss: 0.233826\n",
            "[114]\ttraining's binary_logloss: 0.233226\n",
            "[115]\ttraining's binary_logloss: 0.232635\n",
            "[116]\ttraining's binary_logloss: 0.231986\n",
            "[117]\ttraining's binary_logloss: 0.231419\n",
            "[118]\ttraining's binary_logloss: 0.230858\n",
            "[119]\ttraining's binary_logloss: 0.230313\n",
            "[120]\ttraining's binary_logloss: 0.229571\n",
            "[121]\ttraining's binary_logloss: 0.228982\n",
            "[122]\ttraining's binary_logloss: 0.228405\n",
            "[123]\ttraining's binary_logloss: 0.227882\n",
            "[124]\ttraining's binary_logloss: 0.227329\n",
            "[125]\ttraining's binary_logloss: 0.22662\n",
            "[126]\ttraining's binary_logloss: 0.226012\n",
            "[127]\ttraining's binary_logloss: 0.225479\n",
            "[128]\ttraining's binary_logloss: 0.224878\n",
            "[129]\ttraining's binary_logloss: 0.224321\n",
            "[130]\ttraining's binary_logloss: 0.22372\n",
            "[131]\ttraining's binary_logloss: 0.2232\n",
            "[132]\ttraining's binary_logloss: 0.222681\n",
            "[133]\ttraining's binary_logloss: 0.222118\n",
            "[134]\ttraining's binary_logloss: 0.221521\n",
            "[135]\ttraining's binary_logloss: 0.220775\n",
            "[136]\ttraining's binary_logloss: 0.220231\n",
            "[137]\ttraining's binary_logloss: 0.219751\n",
            "[138]\ttraining's binary_logloss: 0.21907\n",
            "[139]\ttraining's binary_logloss: 0.218457\n",
            "[140]\ttraining's binary_logloss: 0.217921\n",
            "[141]\ttraining's binary_logloss: 0.217398\n",
            "[142]\ttraining's binary_logloss: 0.216908\n",
            "[143]\ttraining's binary_logloss: 0.21641\n",
            "[144]\ttraining's binary_logloss: 0.215791\n",
            "[145]\ttraining's binary_logloss: 0.215311\n",
            "[146]\ttraining's binary_logloss: 0.214843\n",
            "[147]\ttraining's binary_logloss: 0.214359\n",
            "[148]\ttraining's binary_logloss: 0.213829\n",
            "[149]\ttraining's binary_logloss: 0.213242\n",
            "[150]\ttraining's binary_logloss: 0.212679\n",
            "[151]\ttraining's binary_logloss: 0.212125\n",
            "[152]\ttraining's binary_logloss: 0.21165\n",
            "[153]\ttraining's binary_logloss: 0.211105\n",
            "[154]\ttraining's binary_logloss: 0.210591\n",
            "[155]\ttraining's binary_logloss: 0.210105\n",
            "[156]\ttraining's binary_logloss: 0.209579\n",
            "[157]\ttraining's binary_logloss: 0.209068\n",
            "[158]\ttraining's binary_logloss: 0.20854\n",
            "[159]\ttraining's binary_logloss: 0.208065\n",
            "[160]\ttraining's binary_logloss: 0.207545\n",
            "[161]\ttraining's binary_logloss: 0.207029\n",
            "[162]\ttraining's binary_logloss: 0.206549\n",
            "[163]\ttraining's binary_logloss: 0.206132\n",
            "[164]\ttraining's binary_logloss: 0.205732\n",
            "[165]\ttraining's binary_logloss: 0.205298\n",
            "[166]\ttraining's binary_logloss: 0.204843\n",
            "[167]\ttraining's binary_logloss: 0.204319\n",
            "[168]\ttraining's binary_logloss: 0.203842\n",
            "[169]\ttraining's binary_logloss: 0.203404\n",
            "[170]\ttraining's binary_logloss: 0.202774\n",
            "[171]\ttraining's binary_logloss: 0.202253\n",
            "[172]\ttraining's binary_logloss: 0.201816\n",
            "[173]\ttraining's binary_logloss: 0.201415\n",
            "[174]\ttraining's binary_logloss: 0.200899\n",
            "[175]\ttraining's binary_logloss: 0.20051\n",
            "[176]\ttraining's binary_logloss: 0.199975\n",
            "[177]\ttraining's binary_logloss: 0.199501\n",
            "[178]\ttraining's binary_logloss: 0.198932\n",
            "[179]\ttraining's binary_logloss: 0.198436\n",
            "[180]\ttraining's binary_logloss: 0.197943\n",
            "[181]\ttraining's binary_logloss: 0.197429\n",
            "[182]\ttraining's binary_logloss: 0.196968\n",
            "[183]\ttraining's binary_logloss: 0.196531\n",
            "[184]\ttraining's binary_logloss: 0.19614\n",
            "[185]\ttraining's binary_logloss: 0.195741\n",
            "[186]\ttraining's binary_logloss: 0.195312\n",
            "[187]\ttraining's binary_logloss: 0.194823\n",
            "[188]\ttraining's binary_logloss: 0.194335\n",
            "[189]\ttraining's binary_logloss: 0.1939\n",
            "[190]\ttraining's binary_logloss: 0.193505\n",
            "[191]\ttraining's binary_logloss: 0.19307\n",
            "[192]\ttraining's binary_logloss: 0.192633\n",
            "[193]\ttraining's binary_logloss: 0.192295\n",
            "[194]\ttraining's binary_logloss: 0.191856\n",
            "[195]\ttraining's binary_logloss: 0.191352\n",
            "[196]\ttraining's binary_logloss: 0.190937\n",
            "[197]\ttraining's binary_logloss: 0.190602\n",
            "[198]\ttraining's binary_logloss: 0.190188\n",
            "[199]\ttraining's binary_logloss: 0.189725\n",
            "[200]\ttraining's binary_logloss: 0.189204\n",
            "[201]\ttraining's binary_logloss: 0.188791\n",
            "[202]\ttraining's binary_logloss: 0.188392\n",
            "[203]\ttraining's binary_logloss: 0.187954\n",
            "[204]\ttraining's binary_logloss: 0.18743\n",
            "[205]\ttraining's binary_logloss: 0.187087\n",
            "[206]\ttraining's binary_logloss: 0.1866\n",
            "[207]\ttraining's binary_logloss: 0.186156\n",
            "[208]\ttraining's binary_logloss: 0.185705\n",
            "[209]\ttraining's binary_logloss: 0.185275\n",
            "[210]\ttraining's binary_logloss: 0.184777\n",
            "[211]\ttraining's binary_logloss: 0.184351\n",
            "[212]\ttraining's binary_logloss: 0.183936\n",
            "[213]\ttraining's binary_logloss: 0.183536\n",
            "[214]\ttraining's binary_logloss: 0.183149\n",
            "[215]\ttraining's binary_logloss: 0.182794\n",
            "[216]\ttraining's binary_logloss: 0.182343\n",
            "[217]\ttraining's binary_logloss: 0.181963\n",
            "[218]\ttraining's binary_logloss: 0.181571\n",
            "[219]\ttraining's binary_logloss: 0.181169\n",
            "[220]\ttraining's binary_logloss: 0.180767\n",
            "[221]\ttraining's binary_logloss: 0.180375\n",
            "[222]\ttraining's binary_logloss: 0.179908\n",
            "[223]\ttraining's binary_logloss: 0.17954\n",
            "[224]\ttraining's binary_logloss: 0.179073\n",
            "[225]\ttraining's binary_logloss: 0.1787\n",
            "[226]\ttraining's binary_logloss: 0.178296\n",
            "[227]\ttraining's binary_logloss: 0.177897\n",
            "[228]\ttraining's binary_logloss: 0.177508\n",
            "[229]\ttraining's binary_logloss: 0.177196\n",
            "[230]\ttraining's binary_logloss: 0.17683\n",
            "[231]\ttraining's binary_logloss: 0.176455\n",
            "[232]\ttraining's binary_logloss: 0.176096\n",
            "[233]\ttraining's binary_logloss: 0.175723\n",
            "[234]\ttraining's binary_logloss: 0.17535\n",
            "[235]\ttraining's binary_logloss: 0.174945\n",
            "[236]\ttraining's binary_logloss: 0.174501\n",
            "[237]\ttraining's binary_logloss: 0.17408\n",
            "[238]\ttraining's binary_logloss: 0.173689\n",
            "[239]\ttraining's binary_logloss: 0.173284\n",
            "[240]\ttraining's binary_logloss: 0.17293\n",
            "[241]\ttraining's binary_logloss: 0.172478\n",
            "[242]\ttraining's binary_logloss: 0.172071\n",
            "[243]\ttraining's binary_logloss: 0.171651\n",
            "[244]\ttraining's binary_logloss: 0.171275\n",
            "[245]\ttraining's binary_logloss: 0.170912\n",
            "[246]\ttraining's binary_logloss: 0.170571\n",
            "[247]\ttraining's binary_logloss: 0.17021\n",
            "[248]\ttraining's binary_logloss: 0.169853\n",
            "[249]\ttraining's binary_logloss: 0.169543\n",
            "[250]\ttraining's binary_logloss: 0.169214\n",
            "[251]\ttraining's binary_logloss: 0.168875\n",
            "[252]\ttraining's binary_logloss: 0.168513\n",
            "[253]\ttraining's binary_logloss: 0.16818\n",
            "[254]\ttraining's binary_logloss: 0.167766\n",
            "[255]\ttraining's binary_logloss: 0.167448\n",
            "[256]\ttraining's binary_logloss: 0.166971\n",
            "[257]\ttraining's binary_logloss: 0.166603\n",
            "[258]\ttraining's binary_logloss: 0.166239\n",
            "[259]\ttraining's binary_logloss: 0.165839\n",
            "[260]\ttraining's binary_logloss: 0.165422\n",
            "[261]\ttraining's binary_logloss: 0.164974\n",
            "[262]\ttraining's binary_logloss: 0.164627\n",
            "[263]\ttraining's binary_logloss: 0.164338\n",
            "[264]\ttraining's binary_logloss: 0.164023\n",
            "[265]\ttraining's binary_logloss: 0.163708\n",
            "[266]\ttraining's binary_logloss: 0.16337\n",
            "[267]\ttraining's binary_logloss: 0.162985\n",
            "[268]\ttraining's binary_logloss: 0.162575\n",
            "[269]\ttraining's binary_logloss: 0.162167\n",
            "[270]\ttraining's binary_logloss: 0.161824\n",
            "[271]\ttraining's binary_logloss: 0.161519\n",
            "[272]\ttraining's binary_logloss: 0.161205\n",
            "[273]\ttraining's binary_logloss: 0.160784\n",
            "[274]\ttraining's binary_logloss: 0.160349\n",
            "[275]\ttraining's binary_logloss: 0.160049\n",
            "[276]\ttraining's binary_logloss: 0.159681\n",
            "[277]\ttraining's binary_logloss: 0.159299\n",
            "[278]\ttraining's binary_logloss: 0.158951\n",
            "[279]\ttraining's binary_logloss: 0.158665\n",
            "[280]\ttraining's binary_logloss: 0.158332\n",
            "[281]\ttraining's binary_logloss: 0.158012\n",
            "[282]\ttraining's binary_logloss: 0.157595\n",
            "[283]\ttraining's binary_logloss: 0.157252\n",
            "[284]\ttraining's binary_logloss: 0.156821\n",
            "[285]\ttraining's binary_logloss: 0.156519\n",
            "[286]\ttraining's binary_logloss: 0.156218\n",
            "[287]\ttraining's binary_logloss: 0.155897\n",
            "[288]\ttraining's binary_logloss: 0.155576\n",
            "[289]\ttraining's binary_logloss: 0.155247\n",
            "[290]\ttraining's binary_logloss: 0.154865\n",
            "[291]\ttraining's binary_logloss: 0.154509\n",
            "[292]\ttraining's binary_logloss: 0.154244\n",
            "[293]\ttraining's binary_logloss: 0.153896\n",
            "[294]\ttraining's binary_logloss: 0.153556\n",
            "[295]\ttraining's binary_logloss: 0.153306\n",
            "[296]\ttraining's binary_logloss: 0.153011\n",
            "[297]\ttraining's binary_logloss: 0.152629\n",
            "[298]\ttraining's binary_logloss: 0.15227\n",
            "[299]\ttraining's binary_logloss: 0.15199\n",
            "[300]\ttraining's binary_logloss: 0.151738\n",
            "[301]\ttraining's binary_logloss: 0.151382\n",
            "[302]\ttraining's binary_logloss: 0.151072\n",
            "[303]\ttraining's binary_logloss: 0.150786\n",
            "[304]\ttraining's binary_logloss: 0.150511\n",
            "[305]\ttraining's binary_logloss: 0.150099\n",
            "[306]\ttraining's binary_logloss: 0.149764\n",
            "[307]\ttraining's binary_logloss: 0.149391\n",
            "[308]\ttraining's binary_logloss: 0.149069\n",
            "[309]\ttraining's binary_logloss: 0.148788\n",
            "[310]\ttraining's binary_logloss: 0.148492\n",
            "[311]\ttraining's binary_logloss: 0.148134\n",
            "[312]\ttraining's binary_logloss: 0.147817\n",
            "[313]\ttraining's binary_logloss: 0.147538\n",
            "[314]\ttraining's binary_logloss: 0.147187\n",
            "[315]\ttraining's binary_logloss: 0.146863\n",
            "[316]\ttraining's binary_logloss: 0.146486\n",
            "[317]\ttraining's binary_logloss: 0.146225\n",
            "[318]\ttraining's binary_logloss: 0.145934\n",
            "[319]\ttraining's binary_logloss: 0.145645\n",
            "[320]\ttraining's binary_logloss: 0.145347\n",
            "[321]\ttraining's binary_logloss: 0.145015\n",
            "[322]\ttraining's binary_logloss: 0.144653\n",
            "[323]\ttraining's binary_logloss: 0.144349\n",
            "[324]\ttraining's binary_logloss: 0.144073\n",
            "[325]\ttraining's binary_logloss: 0.143841\n",
            "[326]\ttraining's binary_logloss: 0.143467\n",
            "[327]\ttraining's binary_logloss: 0.143159\n",
            "[328]\ttraining's binary_logloss: 0.142834\n",
            "[329]\ttraining's binary_logloss: 0.142558\n",
            "[330]\ttraining's binary_logloss: 0.142276\n",
            "[331]\ttraining's binary_logloss: 0.142033\n",
            "[332]\ttraining's binary_logloss: 0.141716\n",
            "[333]\ttraining's binary_logloss: 0.141389\n",
            "[334]\ttraining's binary_logloss: 0.141106\n",
            "[335]\ttraining's binary_logloss: 0.140826\n",
            "[336]\ttraining's binary_logloss: 0.140402\n",
            "[337]\ttraining's binary_logloss: 0.14007\n",
            "[338]\ttraining's binary_logloss: 0.139828\n",
            "[339]\ttraining's binary_logloss: 0.139591\n",
            "[340]\ttraining's binary_logloss: 0.139307\n",
            "[341]\ttraining's binary_logloss: 0.139044\n",
            "[342]\ttraining's binary_logloss: 0.138642\n",
            "[343]\ttraining's binary_logloss: 0.138412\n",
            "[344]\ttraining's binary_logloss: 0.138094\n",
            "[345]\ttraining's binary_logloss: 0.137807\n",
            "[346]\ttraining's binary_logloss: 0.137524\n",
            "[347]\ttraining's binary_logloss: 0.137266\n",
            "[348]\ttraining's binary_logloss: 0.136974\n",
            "[349]\ttraining's binary_logloss: 0.13675\n",
            "[350]\ttraining's binary_logloss: 0.136476\n",
            "[351]\ttraining's binary_logloss: 0.136118\n",
            "[352]\ttraining's binary_logloss: 0.135836\n",
            "[353]\ttraining's binary_logloss: 0.135523\n",
            "[354]\ttraining's binary_logloss: 0.135276\n",
            "[355]\ttraining's binary_logloss: 0.134999\n",
            "[356]\ttraining's binary_logloss: 0.134653\n",
            "[357]\ttraining's binary_logloss: 0.134413\n",
            "[358]\ttraining's binary_logloss: 0.134143\n",
            "[359]\ttraining's binary_logloss: 0.133834\n",
            "[360]\ttraining's binary_logloss: 0.133522\n",
            "[361]\ttraining's binary_logloss: 0.133253\n",
            "[362]\ttraining's binary_logloss: 0.132954\n",
            "[363]\ttraining's binary_logloss: 0.132672\n",
            "[364]\ttraining's binary_logloss: 0.132346\n",
            "[365]\ttraining's binary_logloss: 0.132115\n",
            "[366]\ttraining's binary_logloss: 0.131805\n",
            "[367]\ttraining's binary_logloss: 0.131579\n",
            "[368]\ttraining's binary_logloss: 0.131297\n",
            "[369]\ttraining's binary_logloss: 0.131031\n",
            "[370]\ttraining's binary_logloss: 0.130803\n",
            "[371]\ttraining's binary_logloss: 0.130527\n",
            "[372]\ttraining's binary_logloss: 0.13025\n",
            "[373]\ttraining's binary_logloss: 0.130043\n",
            "[374]\ttraining's binary_logloss: 0.129813\n",
            "[375]\ttraining's binary_logloss: 0.129531\n",
            "[376]\ttraining's binary_logloss: 0.129297\n",
            "[377]\ttraining's binary_logloss: 0.12901\n",
            "[378]\ttraining's binary_logloss: 0.128723\n",
            "[379]\ttraining's binary_logloss: 0.128327\n",
            "[380]\ttraining's binary_logloss: 0.128035\n",
            "[381]\ttraining's binary_logloss: 0.127702\n",
            "[382]\ttraining's binary_logloss: 0.12749\n",
            "[383]\ttraining's binary_logloss: 0.127217\n",
            "[384]\ttraining's binary_logloss: 0.127002\n",
            "[385]\ttraining's binary_logloss: 0.126798\n",
            "[386]\ttraining's binary_logloss: 0.126493\n",
            "[387]\ttraining's binary_logloss: 0.126284\n",
            "[388]\ttraining's binary_logloss: 0.126044\n",
            "[389]\ttraining's binary_logloss: 0.125797\n",
            "[390]\ttraining's binary_logloss: 0.125557\n",
            "[391]\ttraining's binary_logloss: 0.125305\n",
            "[392]\ttraining's binary_logloss: 0.125066\n",
            "[393]\ttraining's binary_logloss: 0.124815\n",
            "[394]\ttraining's binary_logloss: 0.12449\n",
            "[395]\ttraining's binary_logloss: 0.124177\n",
            "[396]\ttraining's binary_logloss: 0.123906\n",
            "[397]\ttraining's binary_logloss: 0.123583\n",
            "[398]\ttraining's binary_logloss: 0.123356\n",
            "[399]\ttraining's binary_logloss: 0.123034\n",
            "[400]\ttraining's binary_logloss: 0.122833\n",
            "[401]\ttraining's binary_logloss: 0.122624\n",
            "[402]\ttraining's binary_logloss: 0.122432\n",
            "[403]\ttraining's binary_logloss: 0.12212\n",
            "[404]\ttraining's binary_logloss: 0.121896\n",
            "[405]\ttraining's binary_logloss: 0.121567\n",
            "[406]\ttraining's binary_logloss: 0.121318\n",
            "[407]\ttraining's binary_logloss: 0.121064\n",
            "[408]\ttraining's binary_logloss: 0.120873\n",
            "[409]\ttraining's binary_logloss: 0.120642\n",
            "[410]\ttraining's binary_logloss: 0.120403\n",
            "[411]\ttraining's binary_logloss: 0.120182\n",
            "[412]\ttraining's binary_logloss: 0.119887\n",
            "[413]\ttraining's binary_logloss: 0.11968\n",
            "[414]\ttraining's binary_logloss: 0.119455\n",
            "[415]\ttraining's binary_logloss: 0.119181\n",
            "[416]\ttraining's binary_logloss: 0.118985\n",
            "[417]\ttraining's binary_logloss: 0.118657\n",
            "[418]\ttraining's binary_logloss: 0.118448\n",
            "[419]\ttraining's binary_logloss: 0.118178\n",
            "[420]\ttraining's binary_logloss: 0.117963\n",
            "[421]\ttraining's binary_logloss: 0.117767\n",
            "[422]\ttraining's binary_logloss: 0.117589\n",
            "[423]\ttraining's binary_logloss: 0.117415\n",
            "[424]\ttraining's binary_logloss: 0.117241\n",
            "[425]\ttraining's binary_logloss: 0.116943\n",
            "[426]\ttraining's binary_logloss: 0.11667\n",
            "[427]\ttraining's binary_logloss: 0.116429\n",
            "[428]\ttraining's binary_logloss: 0.116224\n",
            "[429]\ttraining's binary_logloss: 0.115947\n",
            "[430]\ttraining's binary_logloss: 0.115588\n",
            "[431]\ttraining's binary_logloss: 0.115273\n",
            "[432]\ttraining's binary_logloss: 0.115081\n",
            "[433]\ttraining's binary_logloss: 0.114853\n",
            "[434]\ttraining's binary_logloss: 0.11458\n",
            "[435]\ttraining's binary_logloss: 0.114362\n",
            "[436]\ttraining's binary_logloss: 0.114158\n",
            "[437]\ttraining's binary_logloss: 0.113868\n",
            "[438]\ttraining's binary_logloss: 0.113598\n",
            "[439]\ttraining's binary_logloss: 0.113358\n",
            "[440]\ttraining's binary_logloss: 0.113162\n",
            "[441]\ttraining's binary_logloss: 0.112854\n",
            "[442]\ttraining's binary_logloss: 0.112618\n",
            "[443]\ttraining's binary_logloss: 0.112446\n",
            "[444]\ttraining's binary_logloss: 0.112185\n",
            "[445]\ttraining's binary_logloss: 0.111892\n",
            "[446]\ttraining's binary_logloss: 0.111628\n",
            "[447]\ttraining's binary_logloss: 0.111339\n",
            "[448]\ttraining's binary_logloss: 0.111079\n",
            "[449]\ttraining's binary_logloss: 0.110781\n",
            "[450]\ttraining's binary_logloss: 0.110533\n",
            "[451]\ttraining's binary_logloss: 0.11032\n",
            "[452]\ttraining's binary_logloss: 0.110136\n",
            "[453]\ttraining's binary_logloss: 0.109968\n",
            "[454]\ttraining's binary_logloss: 0.109689\n",
            "[455]\ttraining's binary_logloss: 0.109526\n",
            "[456]\ttraining's binary_logloss: 0.109246\n",
            "[457]\ttraining's binary_logloss: 0.10908\n",
            "[458]\ttraining's binary_logloss: 0.108833\n",
            "[459]\ttraining's binary_logloss: 0.108551\n",
            "[460]\ttraining's binary_logloss: 0.108332\n",
            "[461]\ttraining's binary_logloss: 0.108137\n",
            "[462]\ttraining's binary_logloss: 0.107844\n",
            "[463]\ttraining's binary_logloss: 0.107664\n",
            "[464]\ttraining's binary_logloss: 0.107494\n",
            "[465]\ttraining's binary_logloss: 0.107292\n",
            "[466]\ttraining's binary_logloss: 0.107078\n",
            "[467]\ttraining's binary_logloss: 0.106857\n",
            "[468]\ttraining's binary_logloss: 0.106592\n",
            "[469]\ttraining's binary_logloss: 0.106394\n",
            "[470]\ttraining's binary_logloss: 0.106188\n",
            "[471]\ttraining's binary_logloss: 0.106028\n",
            "[472]\ttraining's binary_logloss: 0.105771\n",
            "[473]\ttraining's binary_logloss: 0.105537\n",
            "[474]\ttraining's binary_logloss: 0.105329\n",
            "[475]\ttraining's binary_logloss: 0.105107\n",
            "[476]\ttraining's binary_logloss: 0.104893\n",
            "[477]\ttraining's binary_logloss: 0.104715\n",
            "[478]\ttraining's binary_logloss: 0.104467\n",
            "[479]\ttraining's binary_logloss: 0.104313\n",
            "[480]\ttraining's binary_logloss: 0.104119\n",
            "[481]\ttraining's binary_logloss: 0.103843\n",
            "[482]\ttraining's binary_logloss: 0.103614\n",
            "[483]\ttraining's binary_logloss: 0.103394\n",
            "[484]\ttraining's binary_logloss: 0.103163\n",
            "[485]\ttraining's binary_logloss: 0.102934\n",
            "[486]\ttraining's binary_logloss: 0.102741\n",
            "[487]\ttraining's binary_logloss: 0.102486\n",
            "[488]\ttraining's binary_logloss: 0.102241\n",
            "[489]\ttraining's binary_logloss: 0.102053\n",
            "[490]\ttraining's binary_logloss: 0.101851\n",
            "[491]\ttraining's binary_logloss: 0.101588\n",
            "[492]\ttraining's binary_logloss: 0.101353\n",
            "[493]\ttraining's binary_logloss: 0.101158\n",
            "[494]\ttraining's binary_logloss: 0.100979\n",
            "[495]\ttraining's binary_logloss: 0.100789\n",
            "[496]\ttraining's binary_logloss: 0.100629\n",
            "[497]\ttraining's binary_logloss: 0.100476\n",
            "[498]\ttraining's binary_logloss: 0.100308\n",
            "[499]\ttraining's binary_logloss: 0.100096\n",
            "[500]\ttraining's binary_logloss: 0.0998763\n",
            "[501]\ttraining's binary_logloss: 0.099675\n",
            "[502]\ttraining's binary_logloss: 0.0994816\n",
            "[503]\ttraining's binary_logloss: 0.0993135\n",
            "[504]\ttraining's binary_logloss: 0.0991277\n",
            "[505]\ttraining's binary_logloss: 0.0989162\n",
            "[506]\ttraining's binary_logloss: 0.098712\n",
            "[507]\ttraining's binary_logloss: 0.0985198\n",
            "[508]\ttraining's binary_logloss: 0.0983636\n",
            "[509]\ttraining's binary_logloss: 0.0981482\n",
            "[510]\ttraining's binary_logloss: 0.0980006\n",
            "[511]\ttraining's binary_logloss: 0.0978407\n",
            "[512]\ttraining's binary_logloss: 0.0975994\n",
            "[513]\ttraining's binary_logloss: 0.0973573\n",
            "[514]\ttraining's binary_logloss: 0.0971891\n",
            "[515]\ttraining's binary_logloss: 0.0969554\n",
            "[516]\ttraining's binary_logloss: 0.0967001\n",
            "[517]\ttraining's binary_logloss: 0.0965485\n",
            "[518]\ttraining's binary_logloss: 0.0963806\n",
            "[519]\ttraining's binary_logloss: 0.0961339\n",
            "[520]\ttraining's binary_logloss: 0.0958696\n",
            "[521]\ttraining's binary_logloss: 0.0956966\n",
            "[522]\ttraining's binary_logloss: 0.0954832\n",
            "[523]\ttraining's binary_logloss: 0.0953041\n",
            "[524]\ttraining's binary_logloss: 0.0950739\n",
            "[525]\ttraining's binary_logloss: 0.094867\n",
            "[526]\ttraining's binary_logloss: 0.0945884\n",
            "[527]\ttraining's binary_logloss: 0.0943749\n",
            "[528]\ttraining's binary_logloss: 0.0941888\n",
            "[529]\ttraining's binary_logloss: 0.0940557\n",
            "[530]\ttraining's binary_logloss: 0.0938586\n",
            "[531]\ttraining's binary_logloss: 0.0937173\n",
            "[532]\ttraining's binary_logloss: 0.0935741\n",
            "[533]\ttraining's binary_logloss: 0.0933563\n",
            "[534]\ttraining's binary_logloss: 0.0931386\n",
            "[535]\ttraining's binary_logloss: 0.0930113\n",
            "[536]\ttraining's binary_logloss: 0.0928198\n",
            "[537]\ttraining's binary_logloss: 0.0926314\n",
            "[538]\ttraining's binary_logloss: 0.092462\n",
            "[539]\ttraining's binary_logloss: 0.0922505\n",
            "[540]\ttraining's binary_logloss: 0.0919783\n",
            "[541]\ttraining's binary_logloss: 0.0917614\n",
            "[542]\ttraining's binary_logloss: 0.0916254\n",
            "[543]\ttraining's binary_logloss: 0.0913832\n",
            "[544]\ttraining's binary_logloss: 0.0912404\n",
            "[545]\ttraining's binary_logloss: 0.0910298\n",
            "[546]\ttraining's binary_logloss: 0.0908784\n",
            "[547]\ttraining's binary_logloss: 0.0906888\n",
            "[548]\ttraining's binary_logloss: 0.0904835\n",
            "[549]\ttraining's binary_logloss: 0.0902634\n",
            "[550]\ttraining's binary_logloss: 0.0900809\n",
            "[551]\ttraining's binary_logloss: 0.0899277\n",
            "[552]\ttraining's binary_logloss: 0.089752\n",
            "[553]\ttraining's binary_logloss: 0.0895352\n",
            "[554]\ttraining's binary_logloss: 0.0892739\n",
            "[555]\ttraining's binary_logloss: 0.0890506\n",
            "[556]\ttraining's binary_logloss: 0.0888305\n",
            "[557]\ttraining's binary_logloss: 0.088688\n",
            "[558]\ttraining's binary_logloss: 0.0885663\n",
            "[559]\ttraining's binary_logloss: 0.0883677\n",
            "[560]\ttraining's binary_logloss: 0.0881527\n",
            "[561]\ttraining's binary_logloss: 0.0879532\n",
            "[562]\ttraining's binary_logloss: 0.0877793\n",
            "[563]\ttraining's binary_logloss: 0.0875789\n",
            "[564]\ttraining's binary_logloss: 0.0873876\n",
            "[565]\ttraining's binary_logloss: 0.0872423\n",
            "[566]\ttraining's binary_logloss: 0.0870611\n",
            "[567]\ttraining's binary_logloss: 0.0868882\n",
            "[568]\ttraining's binary_logloss: 0.0867595\n",
            "[569]\ttraining's binary_logloss: 0.0866024\n",
            "[570]\ttraining's binary_logloss: 0.0864053\n",
            "[571]\ttraining's binary_logloss: 0.0861922\n",
            "[572]\ttraining's binary_logloss: 0.0860449\n",
            "[573]\ttraining's binary_logloss: 0.085903\n",
            "[574]\ttraining's binary_logloss: 0.085711\n",
            "[575]\ttraining's binary_logloss: 0.0855681\n",
            "[576]\ttraining's binary_logloss: 0.085422\n",
            "[577]\ttraining's binary_logloss: 0.0852287\n",
            "[578]\ttraining's binary_logloss: 0.0850005\n",
            "[579]\ttraining's binary_logloss: 0.0848099\n",
            "[580]\ttraining's binary_logloss: 0.0846394\n",
            "[581]\ttraining's binary_logloss: 0.0844355\n",
            "[582]\ttraining's binary_logloss: 0.0842411\n",
            "[583]\ttraining's binary_logloss: 0.0840941\n",
            "[584]\ttraining's binary_logloss: 0.0839399\n",
            "[585]\ttraining's binary_logloss: 0.0837272\n",
            "[586]\ttraining's binary_logloss: 0.0835914\n",
            "[587]\ttraining's binary_logloss: 0.0834192\n",
            "[588]\ttraining's binary_logloss: 0.083234\n",
            "[589]\ttraining's binary_logloss: 0.0830974\n",
            "[590]\ttraining's binary_logloss: 0.082908\n",
            "[591]\ttraining's binary_logloss: 0.0827189\n",
            "[592]\ttraining's binary_logloss: 0.0825284\n",
            "[593]\ttraining's binary_logloss: 0.0823582\n",
            "[594]\ttraining's binary_logloss: 0.0821521\n",
            "[595]\ttraining's binary_logloss: 0.0820252\n",
            "[596]\ttraining's binary_logloss: 0.081819\n",
            "[597]\ttraining's binary_logloss: 0.0816895\n",
            "[598]\ttraining's binary_logloss: 0.0815277\n",
            "[599]\ttraining's binary_logloss: 0.0813316\n",
            "[600]\ttraining's binary_logloss: 0.0811799\n",
            "[601]\ttraining's binary_logloss: 0.0810317\n",
            "[602]\ttraining's binary_logloss: 0.0808618\n",
            "[603]\ttraining's binary_logloss: 0.0806559\n",
            "[604]\ttraining's binary_logloss: 0.0804969\n",
            "[605]\ttraining's binary_logloss: 0.0803805\n",
            "[606]\ttraining's binary_logloss: 0.0802607\n",
            "[607]\ttraining's binary_logloss: 0.0800804\n",
            "[608]\ttraining's binary_logloss: 0.0799506\n",
            "[609]\ttraining's binary_logloss: 0.0797838\n",
            "[610]\ttraining's binary_logloss: 0.0796209\n",
            "[611]\ttraining's binary_logloss: 0.0794446\n",
            "[612]\ttraining's binary_logloss: 0.0793273\n",
            "[613]\ttraining's binary_logloss: 0.0791578\n",
            "[614]\ttraining's binary_logloss: 0.0789584\n",
            "[615]\ttraining's binary_logloss: 0.0787676\n",
            "[616]\ttraining's binary_logloss: 0.0785698\n",
            "[617]\ttraining's binary_logloss: 0.078385\n",
            "[618]\ttraining's binary_logloss: 0.0782106\n",
            "[619]\ttraining's binary_logloss: 0.0780816\n",
            "[620]\ttraining's binary_logloss: 0.0779609\n",
            "[621]\ttraining's binary_logloss: 0.07783\n",
            "[622]\ttraining's binary_logloss: 0.0776078\n",
            "[623]\ttraining's binary_logloss: 0.0774555\n",
            "[624]\ttraining's binary_logloss: 0.0772471\n",
            "[625]\ttraining's binary_logloss: 0.0770487\n",
            "[626]\ttraining's binary_logloss: 0.076909\n",
            "[627]\ttraining's binary_logloss: 0.0767103\n",
            "[628]\ttraining's binary_logloss: 0.0765618\n",
            "[629]\ttraining's binary_logloss: 0.0763914\n",
            "[630]\ttraining's binary_logloss: 0.0762029\n",
            "[631]\ttraining's binary_logloss: 0.0760642\n",
            "[632]\ttraining's binary_logloss: 0.0759476\n",
            "[633]\ttraining's binary_logloss: 0.0757879\n",
            "[634]\ttraining's binary_logloss: 0.0756367\n",
            "[635]\ttraining's binary_logloss: 0.0754494\n",
            "[636]\ttraining's binary_logloss: 0.0753433\n",
            "[637]\ttraining's binary_logloss: 0.075181\n",
            "[638]\ttraining's binary_logloss: 0.0750136\n",
            "[639]\ttraining's binary_logloss: 0.0748296\n",
            "[640]\ttraining's binary_logloss: 0.0747078\n",
            "[641]\ttraining's binary_logloss: 0.0746015\n",
            "[642]\ttraining's binary_logloss: 0.0744348\n",
            "[643]\ttraining's binary_logloss: 0.0742772\n",
            "[644]\ttraining's binary_logloss: 0.0741275\n",
            "[645]\ttraining's binary_logloss: 0.0739814\n",
            "[646]\ttraining's binary_logloss: 0.07386\n",
            "[647]\ttraining's binary_logloss: 0.0737433\n",
            "[648]\ttraining's binary_logloss: 0.073613\n",
            "[649]\ttraining's binary_logloss: 0.0735077\n",
            "[650]\ttraining's binary_logloss: 0.0733192\n",
            "[651]\ttraining's binary_logloss: 0.0731329\n",
            "[652]\ttraining's binary_logloss: 0.0730199\n",
            "[653]\ttraining's binary_logloss: 0.0728604\n",
            "[654]\ttraining's binary_logloss: 0.0727514\n",
            "[655]\ttraining's binary_logloss: 0.0726095\n",
            "[656]\ttraining's binary_logloss: 0.0724182\n",
            "[657]\ttraining's binary_logloss: 0.0722613\n",
            "[658]\ttraining's binary_logloss: 0.0721297\n",
            "[659]\ttraining's binary_logloss: 0.0720006\n",
            "[660]\ttraining's binary_logloss: 0.0718965\n",
            "[661]\ttraining's binary_logloss: 0.0717116\n",
            "[662]\ttraining's binary_logloss: 0.0716169\n",
            "[663]\ttraining's binary_logloss: 0.0714744\n",
            "[664]\ttraining's binary_logloss: 0.0713088\n",
            "[665]\ttraining's binary_logloss: 0.0711697\n",
            "[666]\ttraining's binary_logloss: 0.0709988\n",
            "[667]\ttraining's binary_logloss: 0.0708892\n",
            "[668]\ttraining's binary_logloss: 0.0707804\n",
            "[669]\ttraining's binary_logloss: 0.0705909\n",
            "[670]\ttraining's binary_logloss: 0.0704925\n",
            "[671]\ttraining's binary_logloss: 0.0703087\n",
            "[672]\ttraining's binary_logloss: 0.0702073\n",
            "[673]\ttraining's binary_logloss: 0.0700735\n",
            "[674]\ttraining's binary_logloss: 0.0699469\n",
            "[675]\ttraining's binary_logloss: 0.0697863\n",
            "[676]\ttraining's binary_logloss: 0.0696642\n",
            "[677]\ttraining's binary_logloss: 0.0695338\n",
            "[678]\ttraining's binary_logloss: 0.0693513\n",
            "[679]\ttraining's binary_logloss: 0.0691784\n",
            "[680]\ttraining's binary_logloss: 0.06908\n",
            "[681]\ttraining's binary_logloss: 0.0689253\n",
            "[682]\ttraining's binary_logloss: 0.0687633\n",
            "[683]\ttraining's binary_logloss: 0.0686542\n",
            "[684]\ttraining's binary_logloss: 0.0685008\n",
            "[685]\ttraining's binary_logloss: 0.0683329\n",
            "[686]\ttraining's binary_logloss: 0.0681595\n",
            "[687]\ttraining's binary_logloss: 0.0680016\n",
            "[688]\ttraining's binary_logloss: 0.0678871\n",
            "[689]\ttraining's binary_logloss: 0.0677506\n",
            "[690]\ttraining's binary_logloss: 0.0676121\n",
            "[691]\ttraining's binary_logloss: 0.0675108\n",
            "[692]\ttraining's binary_logloss: 0.067391\n",
            "[693]\ttraining's binary_logloss: 0.0672389\n",
            "[694]\ttraining's binary_logloss: 0.0671236\n",
            "[695]\ttraining's binary_logloss: 0.0669801\n",
            "[696]\ttraining's binary_logloss: 0.0668263\n",
            "[697]\ttraining's binary_logloss: 0.0666521\n",
            "[698]\ttraining's binary_logloss: 0.0665465\n",
            "[699]\ttraining's binary_logloss: 0.0664391\n",
            "[700]\ttraining's binary_logloss: 0.066334\n",
            "[701]\ttraining's binary_logloss: 0.066219\n",
            "[702]\ttraining's binary_logloss: 0.0660372\n",
            "[703]\ttraining's binary_logloss: 0.0659442\n",
            "[704]\ttraining's binary_logloss: 0.0657948\n",
            "[705]\ttraining's binary_logloss: 0.0656327\n",
            "[706]\ttraining's binary_logloss: 0.0655395\n",
            "[707]\ttraining's binary_logloss: 0.0653889\n",
            "[708]\ttraining's binary_logloss: 0.0652097\n",
            "[709]\ttraining's binary_logloss: 0.0650598\n",
            "[710]\ttraining's binary_logloss: 0.0649138\n",
            "[711]\ttraining's binary_logloss: 0.0647443\n",
            "[712]\ttraining's binary_logloss: 0.0645923\n",
            "[713]\ttraining's binary_logloss: 0.0644689\n",
            "[714]\ttraining's binary_logloss: 0.0643311\n",
            "[715]\ttraining's binary_logloss: 0.0641974\n",
            "[716]\ttraining's binary_logloss: 0.0640418\n",
            "[717]\ttraining's binary_logloss: 0.0639455\n",
            "[718]\ttraining's binary_logloss: 0.063844\n",
            "[719]\ttraining's binary_logloss: 0.0636804\n",
            "[720]\ttraining's binary_logloss: 0.0635779\n",
            "[721]\ttraining's binary_logloss: 0.0634835\n",
            "[722]\ttraining's binary_logloss: 0.0633202\n",
            "[723]\ttraining's binary_logloss: 0.0632068\n",
            "[724]\ttraining's binary_logloss: 0.0630561\n",
            "[725]\ttraining's binary_logloss: 0.0629007\n",
            "[726]\ttraining's binary_logloss: 0.0627861\n",
            "[727]\ttraining's binary_logloss: 0.0626895\n",
            "[728]\ttraining's binary_logloss: 0.0625883\n",
            "[729]\ttraining's binary_logloss: 0.0624896\n",
            "[730]\ttraining's binary_logloss: 0.0623777\n",
            "[731]\ttraining's binary_logloss: 0.0622797\n",
            "[732]\ttraining's binary_logloss: 0.0621433\n",
            "[733]\ttraining's binary_logloss: 0.0620537\n",
            "[734]\ttraining's binary_logloss: 0.0618897\n",
            "[735]\ttraining's binary_logloss: 0.061754\n",
            "[736]\ttraining's binary_logloss: 0.0616566\n",
            "[737]\ttraining's binary_logloss: 0.0615173\n",
            "[738]\ttraining's binary_logloss: 0.0614022\n",
            "[739]\ttraining's binary_logloss: 0.0612706\n",
            "[740]\ttraining's binary_logloss: 0.0610975\n",
            "[741]\ttraining's binary_logloss: 0.0609935\n",
            "[742]\ttraining's binary_logloss: 0.0608549\n",
            "[743]\ttraining's binary_logloss: 0.0607616\n",
            "[744]\ttraining's binary_logloss: 0.0606677\n",
            "[745]\ttraining's binary_logloss: 0.060533\n",
            "[746]\ttraining's binary_logloss: 0.060385\n",
            "[747]\ttraining's binary_logloss: 0.0602395\n",
            "[748]\ttraining's binary_logloss: 0.0601244\n",
            "[749]\ttraining's binary_logloss: 0.0600191\n",
            "[750]\ttraining's binary_logloss: 0.0599191\n",
            "[751]\ttraining's binary_logloss: 0.0597585\n",
            "[752]\ttraining's binary_logloss: 0.0596192\n",
            "[753]\ttraining's binary_logloss: 0.0594827\n",
            "[754]\ttraining's binary_logloss: 0.0594004\n",
            "[755]\ttraining's binary_logloss: 0.0592676\n",
            "[756]\ttraining's binary_logloss: 0.0591165\n",
            "[757]\ttraining's binary_logloss: 0.0589754\n",
            "[758]\ttraining's binary_logloss: 0.0588437\n",
            "[759]\ttraining's binary_logloss: 0.0587202\n",
            "[760]\ttraining's binary_logloss: 0.0585863\n",
            "[761]\ttraining's binary_logloss: 0.0584634\n",
            "[762]\ttraining's binary_logloss: 0.0583822\n",
            "[763]\ttraining's binary_logloss: 0.0582599\n",
            "[764]\ttraining's binary_logloss: 0.0581268\n",
            "[765]\ttraining's binary_logloss: 0.0580099\n",
            "[766]\ttraining's binary_logloss: 0.0579055\n",
            "[767]\ttraining's binary_logloss: 0.0578001\n",
            "[768]\ttraining's binary_logloss: 0.0576974\n",
            "[769]\ttraining's binary_logloss: 0.0576055\n",
            "[770]\ttraining's binary_logloss: 0.0574919\n",
            "[771]\ttraining's binary_logloss: 0.0573447\n",
            "[772]\ttraining's binary_logloss: 0.0572046\n",
            "[773]\ttraining's binary_logloss: 0.0571125\n",
            "[774]\ttraining's binary_logloss: 0.0569805\n",
            "[775]\ttraining's binary_logloss: 0.0568844\n",
            "[776]\ttraining's binary_logloss: 0.0567746\n",
            "[777]\ttraining's binary_logloss: 0.0566392\n",
            "[778]\ttraining's binary_logloss: 0.056506\n",
            "[779]\ttraining's binary_logloss: 0.0564095\n",
            "[780]\ttraining's binary_logloss: 0.0563188\n",
            "[781]\ttraining's binary_logloss: 0.0562292\n",
            "[782]\ttraining's binary_logloss: 0.0560943\n",
            "[783]\ttraining's binary_logloss: 0.0560206\n",
            "[784]\ttraining's binary_logloss: 0.0558911\n",
            "[785]\ttraining's binary_logloss: 0.0557931\n",
            "[786]\ttraining's binary_logloss: 0.0556655\n",
            "[787]\ttraining's binary_logloss: 0.0555333\n",
            "[788]\ttraining's binary_logloss: 0.0554183\n",
            "[789]\ttraining's binary_logloss: 0.0552951\n",
            "[790]\ttraining's binary_logloss: 0.0551503\n",
            "[791]\ttraining's binary_logloss: 0.0550215\n",
            "[792]\ttraining's binary_logloss: 0.0549465\n",
            "[793]\ttraining's binary_logloss: 0.0548806\n",
            "[794]\ttraining's binary_logloss: 0.0547578\n",
            "[795]\ttraining's binary_logloss: 0.0546609\n",
            "[796]\ttraining's binary_logloss: 0.0545848\n",
            "[797]\ttraining's binary_logloss: 0.0544576\n",
            "[798]\ttraining's binary_logloss: 0.0543666\n",
            "[799]\ttraining's binary_logloss: 0.0542493\n",
            "[800]\ttraining's binary_logloss: 0.054117\n",
            "[801]\ttraining's binary_logloss: 0.053974\n",
            "[802]\ttraining's binary_logloss: 0.0538813\n",
            "[803]\ttraining's binary_logloss: 0.053764\n",
            "[804]\ttraining's binary_logloss: 0.0536429\n",
            "[805]\ttraining's binary_logloss: 0.0535684\n",
            "[806]\ttraining's binary_logloss: 0.0534323\n",
            "[807]\ttraining's binary_logloss: 0.0533148\n",
            "[808]\ttraining's binary_logloss: 0.0532104\n",
            "[809]\ttraining's binary_logloss: 0.0531115\n",
            "[810]\ttraining's binary_logloss: 0.0530215\n",
            "[811]\ttraining's binary_logloss: 0.0529324\n",
            "[812]\ttraining's binary_logloss: 0.0528745\n",
            "[813]\ttraining's binary_logloss: 0.0527853\n",
            "[814]\ttraining's binary_logloss: 0.0526777\n",
            "[815]\ttraining's binary_logloss: 0.0525447\n",
            "[816]\ttraining's binary_logloss: 0.0523941\n",
            "[817]\ttraining's binary_logloss: 0.0522711\n",
            "[818]\ttraining's binary_logloss: 0.0521481\n",
            "[819]\ttraining's binary_logloss: 0.0520298\n",
            "[820]\ttraining's binary_logloss: 0.0519384\n",
            "[821]\ttraining's binary_logloss: 0.0518275\n",
            "[822]\ttraining's binary_logloss: 0.0517408\n",
            "[823]\ttraining's binary_logloss: 0.0516724\n",
            "[824]\ttraining's binary_logloss: 0.0515686\n",
            "[825]\ttraining's binary_logloss: 0.0514963\n",
            "[826]\ttraining's binary_logloss: 0.0514074\n",
            "[827]\ttraining's binary_logloss: 0.0513289\n",
            "[828]\ttraining's binary_logloss: 0.0512528\n",
            "[829]\ttraining's binary_logloss: 0.0511935\n",
            "[830]\ttraining's binary_logloss: 0.0510829\n",
            "[831]\ttraining's binary_logloss: 0.050952\n",
            "[832]\ttraining's binary_logloss: 0.0508728\n",
            "[833]\ttraining's binary_logloss: 0.050804\n",
            "[834]\ttraining's binary_logloss: 0.0506878\n",
            "[835]\ttraining's binary_logloss: 0.0505768\n",
            "[836]\ttraining's binary_logloss: 0.0504814\n",
            "[837]\ttraining's binary_logloss: 0.0503621\n",
            "[838]\ttraining's binary_logloss: 0.050293\n",
            "[839]\ttraining's binary_logloss: 0.0501941\n",
            "[840]\ttraining's binary_logloss: 0.0501231\n",
            "[841]\ttraining's binary_logloss: 0.0500555\n",
            "[842]\ttraining's binary_logloss: 0.0499296\n",
            "[843]\ttraining's binary_logloss: 0.0498143\n",
            "[844]\ttraining's binary_logloss: 0.0496813\n",
            "[845]\ttraining's binary_logloss: 0.0495652\n",
            "[846]\ttraining's binary_logloss: 0.0494577\n",
            "[847]\ttraining's binary_logloss: 0.0493631\n",
            "[848]\ttraining's binary_logloss: 0.0492783\n",
            "[849]\ttraining's binary_logloss: 0.0491564\n",
            "[850]\ttraining's binary_logloss: 0.0490433\n",
            "[851]\ttraining's binary_logloss: 0.0489547\n",
            "[852]\ttraining's binary_logloss: 0.0488603\n",
            "[853]\ttraining's binary_logloss: 0.0488012\n",
            "[854]\ttraining's binary_logloss: 0.0487178\n",
            "[855]\ttraining's binary_logloss: 0.0486032\n",
            "[856]\ttraining's binary_logloss: 0.0484799\n",
            "[857]\ttraining's binary_logloss: 0.0484191\n",
            "[858]\ttraining's binary_logloss: 0.04831\n",
            "[859]\ttraining's binary_logloss: 0.0482085\n",
            "[860]\ttraining's binary_logloss: 0.0481012\n",
            "[861]\ttraining's binary_logloss: 0.0480183\n",
            "[862]\ttraining's binary_logloss: 0.0479387\n",
            "[863]\ttraining's binary_logloss: 0.0478573\n",
            "[864]\ttraining's binary_logloss: 0.0477848\n",
            "[865]\ttraining's binary_logloss: 0.0476843\n",
            "[866]\ttraining's binary_logloss: 0.0475928\n",
            "[867]\ttraining's binary_logloss: 0.0474818\n",
            "[868]\ttraining's binary_logloss: 0.0473596\n",
            "[869]\ttraining's binary_logloss: 0.0472489\n",
            "[870]\ttraining's binary_logloss: 0.0471394\n",
            "[871]\ttraining's binary_logloss: 0.0470763\n",
            "[872]\ttraining's binary_logloss: 0.0469984\n",
            "[873]\ttraining's binary_logloss: 0.0469218\n",
            "[874]\ttraining's binary_logloss: 0.0468209\n",
            "[875]\ttraining's binary_logloss: 0.046723\n",
            "[876]\ttraining's binary_logloss: 0.0466533\n",
            "[877]\ttraining's binary_logloss: 0.0465383\n",
            "[878]\ttraining's binary_logloss: 0.0464241\n",
            "[879]\ttraining's binary_logloss: 0.0463553\n",
            "[880]\ttraining's binary_logloss: 0.0462544\n",
            "[881]\ttraining's binary_logloss: 0.0461812\n",
            "[882]\ttraining's binary_logloss: 0.0460924\n",
            "[883]\ttraining's binary_logloss: 0.0460092\n",
            "[884]\ttraining's binary_logloss: 0.0459061\n",
            "[885]\ttraining's binary_logloss: 0.0458368\n",
            "[886]\ttraining's binary_logloss: 0.0457532\n",
            "[887]\ttraining's binary_logloss: 0.0456562\n",
            "[888]\ttraining's binary_logloss: 0.045573\n",
            "[889]\ttraining's binary_logloss: 0.0454951\n",
            "[890]\ttraining's binary_logloss: 0.0453834\n",
            "[891]\ttraining's binary_logloss: 0.0452699\n",
            "[892]\ttraining's binary_logloss: 0.0452004\n",
            "[893]\ttraining's binary_logloss: 0.0451244\n",
            "[894]\ttraining's binary_logloss: 0.0450249\n",
            "[895]\ttraining's binary_logloss: 0.0449575\n",
            "[896]\ttraining's binary_logloss: 0.0448845\n",
            "[897]\ttraining's binary_logloss: 0.0447849\n",
            "[898]\ttraining's binary_logloss: 0.0447167\n",
            "[899]\ttraining's binary_logloss: 0.0446207\n",
            "[900]\ttraining's binary_logloss: 0.0445099\n",
            "[901]\ttraining's binary_logloss: 0.0444402\n",
            "[902]\ttraining's binary_logloss: 0.0443301\n",
            "[903]\ttraining's binary_logloss: 0.0442188\n",
            "[904]\ttraining's binary_logloss: 0.0441343\n",
            "[905]\ttraining's binary_logloss: 0.0440235\n",
            "[906]\ttraining's binary_logloss: 0.0439485\n",
            "[907]\ttraining's binary_logloss: 0.0438591\n",
            "[908]\ttraining's binary_logloss: 0.0437784\n",
            "[909]\ttraining's binary_logloss: 0.0436662\n",
            "[910]\ttraining's binary_logloss: 0.0435954\n",
            "[911]\ttraining's binary_logloss: 0.0435062\n",
            "[912]\ttraining's binary_logloss: 0.0434351\n",
            "[913]\ttraining's binary_logloss: 0.0433573\n",
            "[914]\ttraining's binary_logloss: 0.0432908\n",
            "[915]\ttraining's binary_logloss: 0.0432011\n",
            "[916]\ttraining's binary_logloss: 0.0431275\n",
            "[917]\ttraining's binary_logloss: 0.0430732\n",
            "[918]\ttraining's binary_logloss: 0.0429933\n",
            "[919]\ttraining's binary_logloss: 0.0429032\n",
            "[920]\ttraining's binary_logloss: 0.0428474\n",
            "[921]\ttraining's binary_logloss: 0.042783\n",
            "[922]\ttraining's binary_logloss: 0.0426883\n",
            "[923]\ttraining's binary_logloss: 0.0425747\n",
            "[924]\ttraining's binary_logloss: 0.0424697\n",
            "[925]\ttraining's binary_logloss: 0.0423729\n",
            "[926]\ttraining's binary_logloss: 0.0423224\n",
            "[927]\ttraining's binary_logloss: 0.0422244\n",
            "[928]\ttraining's binary_logloss: 0.0421603\n",
            "[929]\ttraining's binary_logloss: 0.0420998\n",
            "[930]\ttraining's binary_logloss: 0.0420403\n",
            "[931]\ttraining's binary_logloss: 0.0419645\n",
            "[932]\ttraining's binary_logloss: 0.0419045\n",
            "[933]\ttraining's binary_logloss: 0.0418352\n",
            "[934]\ttraining's binary_logloss: 0.0417514\n",
            "[935]\ttraining's binary_logloss: 0.041645\n",
            "[936]\ttraining's binary_logloss: 0.0415522\n",
            "[937]\ttraining's binary_logloss: 0.0414746\n",
            "[938]\ttraining's binary_logloss: 0.0413848\n",
            "[939]\ttraining's binary_logloss: 0.0412818\n",
            "[940]\ttraining's binary_logloss: 0.041167\n",
            "[941]\ttraining's binary_logloss: 0.041103\n",
            "[942]\ttraining's binary_logloss: 0.0410049\n",
            "[943]\ttraining's binary_logloss: 0.0409193\n",
            "[944]\ttraining's binary_logloss: 0.0408562\n",
            "[945]\ttraining's binary_logloss: 0.0407693\n",
            "[946]\ttraining's binary_logloss: 0.0407076\n",
            "[947]\ttraining's binary_logloss: 0.0406461\n",
            "[948]\ttraining's binary_logloss: 0.040589\n",
            "[949]\ttraining's binary_logloss: 0.0405195\n",
            "[950]\ttraining's binary_logloss: 0.0404616\n",
            "[951]\ttraining's binary_logloss: 0.0403797\n",
            "[952]\ttraining's binary_logloss: 0.0403256\n",
            "[953]\ttraining's binary_logloss: 0.0402563\n",
            "[954]\ttraining's binary_logloss: 0.0401746\n",
            "[955]\ttraining's binary_logloss: 0.0401134\n",
            "[956]\ttraining's binary_logloss: 0.0400545\n",
            "[957]\ttraining's binary_logloss: 0.0399926\n",
            "[958]\ttraining's binary_logloss: 0.039909\n",
            "[959]\ttraining's binary_logloss: 0.0398031\n",
            "[960]\ttraining's binary_logloss: 0.039708\n",
            "[961]\ttraining's binary_logloss: 0.0396559\n",
            "[962]\ttraining's binary_logloss: 0.0395598\n",
            "[963]\ttraining's binary_logloss: 0.0395022\n",
            "[964]\ttraining's binary_logloss: 0.0394453\n",
            "[965]\ttraining's binary_logloss: 0.0393717\n",
            "[966]\ttraining's binary_logloss: 0.0392693\n",
            "[967]\ttraining's binary_logloss: 0.0392057\n",
            "[968]\ttraining's binary_logloss: 0.0391542\n",
            "[969]\ttraining's binary_logloss: 0.0390866\n",
            "[970]\ttraining's binary_logloss: 0.0390361\n",
            "[971]\ttraining's binary_logloss: 0.0389411\n",
            "[972]\ttraining's binary_logloss: 0.0388868\n",
            "[973]\ttraining's binary_logloss: 0.0388169\n",
            "[974]\ttraining's binary_logloss: 0.0387166\n",
            "[975]\ttraining's binary_logloss: 0.0386316\n",
            "[976]\ttraining's binary_logloss: 0.0385664\n",
            "[977]\ttraining's binary_logloss: 0.0385191\n",
            "[978]\ttraining's binary_logloss: 0.0384534\n",
            "[979]\ttraining's binary_logloss: 0.0383999\n",
            "[980]\ttraining's binary_logloss: 0.0383127\n",
            "[981]\ttraining's binary_logloss: 0.0382534\n",
            "[982]\ttraining's binary_logloss: 0.0381876\n",
            "[983]\ttraining's binary_logloss: 0.0380904\n",
            "[984]\ttraining's binary_logloss: 0.0380356\n",
            "[985]\ttraining's binary_logloss: 0.0379726\n",
            "[986]\ttraining's binary_logloss: 0.0379102\n",
            "[987]\ttraining's binary_logloss: 0.0378228\n",
            "[988]\ttraining's binary_logloss: 0.0377476\n",
            "[989]\ttraining's binary_logloss: 0.0376576\n",
            "[990]\ttraining's binary_logloss: 0.0375597\n",
            "[991]\ttraining's binary_logloss: 0.0374855\n",
            "[992]\ttraining's binary_logloss: 0.0374278\n",
            "[993]\ttraining's binary_logloss: 0.0373667\n",
            "[994]\ttraining's binary_logloss: 0.0373159\n",
            "[995]\ttraining's binary_logloss: 0.0372461\n",
            "[996]\ttraining's binary_logloss: 0.0371671\n",
            "[997]\ttraining's binary_logloss: 0.0371061\n",
            "[998]\ttraining's binary_logloss: 0.0370498\n",
            "[999]\ttraining's binary_logloss: 0.0369999\n",
            "[1000]\ttraining's binary_logloss: 0.0369566\n",
            "22.62212634086609\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1URbe3Ua6AZ",
        "colab_type": "text"
      },
      "source": [
        "# 3. Customer Satisfaction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wo8C8rociM_M",
        "colab_type": "code",
        "outputId": "daade846-cc64-497a-863a-48ed053dd24f",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f4e2cb71-3e67-4168-8bcf-e07887d23a77\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-f4e2cb71-3e67-4168-8bcf-e07887d23a77\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving customer_satisfaction.csv to customer_satisfaction.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz10nuqBiNBe",
        "colab_type": "code",
        "outputId": "8047b164-5a35-40ac-ab75-0e0a6b2c35cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(io.BytesIO(uploaded['customer_satisfaction.csv']))\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>var3</th>\n",
              "      <th>var15</th>\n",
              "      <th>imp_ent_var16_ult1</th>\n",
              "      <th>imp_op_var39_comer_ult1</th>\n",
              "      <th>imp_op_var39_comer_ult3</th>\n",
              "      <th>imp_op_var40_comer_ult1</th>\n",
              "      <th>imp_op_var40_comer_ult3</th>\n",
              "      <th>imp_op_var40_efect_ult1</th>\n",
              "      <th>imp_op_var40_efect_ult3</th>\n",
              "      <th>imp_op_var40_ult1</th>\n",
              "      <th>imp_op_var41_comer_ult1</th>\n",
              "      <th>imp_op_var41_comer_ult3</th>\n",
              "      <th>imp_op_var41_efect_ult1</th>\n",
              "      <th>imp_op_var41_efect_ult3</th>\n",
              "      <th>imp_op_var41_ult1</th>\n",
              "      <th>imp_op_var39_efect_ult1</th>\n",
              "      <th>imp_op_var39_efect_ult3</th>\n",
              "      <th>imp_op_var39_ult1</th>\n",
              "      <th>imp_sal_var16_ult1</th>\n",
              "      <th>ind_var1_0</th>\n",
              "      <th>ind_var1</th>\n",
              "      <th>ind_var2_0</th>\n",
              "      <th>ind_var2</th>\n",
              "      <th>ind_var5_0</th>\n",
              "      <th>ind_var5</th>\n",
              "      <th>ind_var6_0</th>\n",
              "      <th>ind_var6</th>\n",
              "      <th>ind_var8_0</th>\n",
              "      <th>ind_var8</th>\n",
              "      <th>ind_var12_0</th>\n",
              "      <th>ind_var12</th>\n",
              "      <th>ind_var13_0</th>\n",
              "      <th>ind_var13_corto_0</th>\n",
              "      <th>ind_var13_corto</th>\n",
              "      <th>ind_var13_largo_0</th>\n",
              "      <th>ind_var13_largo</th>\n",
              "      <th>ind_var13_medio_0</th>\n",
              "      <th>ind_var13_medio</th>\n",
              "      <th>ind_var13</th>\n",
              "      <th>...</th>\n",
              "      <th>saldo_medio_var5_ult1</th>\n",
              "      <th>saldo_medio_var5_ult3</th>\n",
              "      <th>saldo_medio_var8_hace2</th>\n",
              "      <th>saldo_medio_var8_hace3</th>\n",
              "      <th>saldo_medio_var8_ult1</th>\n",
              "      <th>saldo_medio_var8_ult3</th>\n",
              "      <th>saldo_medio_var12_hace2</th>\n",
              "      <th>saldo_medio_var12_hace3</th>\n",
              "      <th>saldo_medio_var12_ult1</th>\n",
              "      <th>saldo_medio_var12_ult3</th>\n",
              "      <th>saldo_medio_var13_corto_hace2</th>\n",
              "      <th>saldo_medio_var13_corto_hace3</th>\n",
              "      <th>saldo_medio_var13_corto_ult1</th>\n",
              "      <th>saldo_medio_var13_corto_ult3</th>\n",
              "      <th>saldo_medio_var13_largo_hace2</th>\n",
              "      <th>saldo_medio_var13_largo_hace3</th>\n",
              "      <th>saldo_medio_var13_largo_ult1</th>\n",
              "      <th>saldo_medio_var13_largo_ult3</th>\n",
              "      <th>saldo_medio_var13_medio_hace2</th>\n",
              "      <th>saldo_medio_var13_medio_hace3</th>\n",
              "      <th>saldo_medio_var13_medio_ult1</th>\n",
              "      <th>saldo_medio_var13_medio_ult3</th>\n",
              "      <th>saldo_medio_var17_hace2</th>\n",
              "      <th>saldo_medio_var17_hace3</th>\n",
              "      <th>saldo_medio_var17_ult1</th>\n",
              "      <th>saldo_medio_var17_ult3</th>\n",
              "      <th>saldo_medio_var29_hace2</th>\n",
              "      <th>saldo_medio_var29_hace3</th>\n",
              "      <th>saldo_medio_var29_ult1</th>\n",
              "      <th>saldo_medio_var29_ult3</th>\n",
              "      <th>saldo_medio_var33_hace2</th>\n",
              "      <th>saldo_medio_var33_hace3</th>\n",
              "      <th>saldo_medio_var33_ult1</th>\n",
              "      <th>saldo_medio_var33_ult3</th>\n",
              "      <th>saldo_medio_var44_hace2</th>\n",
              "      <th>saldo_medio_var44_hace3</th>\n",
              "      <th>saldo_medio_var44_ult1</th>\n",
              "      <th>saldo_medio_var44_ult3</th>\n",
              "      <th>var38</th>\n",
              "      <th>TARGET</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39205.170000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>34</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>300.0</td>\n",
              "      <td>122.22</td>\n",
              "      <td>300.0</td>\n",
              "      <td>240.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>49278.030000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>23</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3.00</td>\n",
              "      <td>2.07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>67333.770000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8</td>\n",
              "      <td>2</td>\n",
              "      <td>37</td>\n",
              "      <td>0.0</td>\n",
              "      <td>195.0</td>\n",
              "      <td>195.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>195.0</td>\n",
              "      <td>195.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>195.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>195.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>91.56</td>\n",
              "      <td>138.84</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64007.970000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>10</td>\n",
              "      <td>2</td>\n",
              "      <td>39</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>40501.08</td>\n",
              "      <td>13501.47</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>85501.89</td>\n",
              "      <td>85501.89</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117310.979016</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 371 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   ID  var3  var15  ...  saldo_medio_var44_ult3          var38  TARGET\n",
              "0   1     2     23  ...                     0.0   39205.170000       0\n",
              "1   3     2     34  ...                     0.0   49278.030000       0\n",
              "2   4     2     23  ...                     0.0   67333.770000       0\n",
              "3   8     2     37  ...                     0.0   64007.970000       0\n",
              "4  10     2     39  ...                     0.0  117310.979016       0\n",
              "\n",
              "[5 rows x 371 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn9KCXCLiNHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get X and y\n",
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6VXcUhPkmkz",
        "colab_type": "code",
        "outputId": "1cd3b578-fb92-4a87-bb60-30d86191948a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Split data into training and testing\n",
        "from sklearn import model_selection\n",
        "\n",
        "# Reserve 20% for testing\n",
        "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.2)\n",
        "\n",
        "print('Training data has %d observation with %d features' % X_train.shape)\n",
        "print('Test data has %d observation with %d features' % X_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data has 60816 observation with 370 features\n",
            "Test data has 15204 observation with 370 features\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OVF4s1vm9G5",
        "colab_type": "text"
      },
      "source": [
        "## 3.1 CatBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jATbZJISmusl",
        "colab_type": "text"
      },
      "source": [
        "### Train in CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCGJbUtqkyBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from catboost import CatBoostClassifier\n",
        "classifier_cat = CatBoostClassifier(iterations = 100, task_type = 'GPU')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvOkl5Pskmvj",
        "colab_type": "code",
        "outputId": "7aa1554b-d466-4a8c-a70a-e13fd4b2ddca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "import timeit\n",
        "def train_on_cpu():  \n",
        "  model = CatBoostClassifier(\n",
        "    iterations=1000,\n",
        "    learning_rate=0.03\n",
        "  )\n",
        "  \n",
        "  model.fit(\n",
        "      X_train, y_train,\n",
        "      eval_set=(X_test, y_test),\n",
        "      verbose=100\n",
        "  );   \n",
        "      \n",
        "cpu_time = timeit.timeit('train_on_cpu()', \n",
        "                         setup=\"from __main__ import train_on_cpu\", \n",
        "                         number=1)\n",
        "\n",
        "print('Time to fit model on CPU: {} sec'.format(int(cpu_time)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0:\tlearn: 0.6499851\ttest: 0.6499793\tbest: 0.6499793 (0)\ttotal: 195ms\tremaining: 3m 14s\n",
            "100:\tlearn: 0.1366798\ttest: 0.1390385\tbest: 0.1390385 (100)\ttotal: 11s\tremaining: 1m 38s\n",
            "200:\tlearn: 0.1323332\ttest: 0.1365640\tbest: 0.1365614 (196)\ttotal: 21.5s\tremaining: 1m 25s\n",
            "300:\tlearn: 0.1304998\ttest: 0.1360108\tbest: 0.1360085 (299)\ttotal: 31.4s\tremaining: 1m 12s\n",
            "400:\tlearn: 0.1289568\ttest: 0.1356632\tbest: 0.1356632 (400)\ttotal: 41.4s\tremaining: 1m 1s\n",
            "500:\tlearn: 0.1274309\ttest: 0.1355078\tbest: 0.1355078 (500)\ttotal: 51.5s\tremaining: 51.3s\n",
            "600:\tlearn: 0.1260395\ttest: 0.1354352\tbest: 0.1354352 (600)\ttotal: 1m 1s\tremaining: 41.1s\n",
            "700:\tlearn: 0.1248485\ttest: 0.1353525\tbest: 0.1353525 (700)\ttotal: 1m 12s\tremaining: 30.8s\n",
            "800:\tlearn: 0.1236035\ttest: 0.1353367\tbest: 0.1353167 (781)\ttotal: 1m 23s\tremaining: 20.6s\n",
            "900:\tlearn: 0.1224571\ttest: 0.1352741\tbest: 0.1352424 (876)\ttotal: 1m 33s\tremaining: 10.3s\n",
            "999:\tlearn: 0.1212619\ttest: 0.1352166\tbest: 0.1351861 (984)\ttotal: 1m 44s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.1351860759\n",
            "bestIteration = 984\n",
            "\n",
            "Shrink model to first 985 iterations.\n",
            "Time to fit model on CPU: 109 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdRJqSPSm0E_",
        "colab_type": "text"
      },
      "source": [
        "### Train in GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqEG7Oyrkmyi",
        "colab_type": "code",
        "outputId": "4cd911ec-a867-4f75-da05-f94b409e62d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "def train_on_gpu():  \n",
        "  model = CatBoostClassifier(\n",
        "    iterations=1000,\n",
        "    learning_rate=0.03,\n",
        "    task_type='GPU'\n",
        "  )\n",
        "  \n",
        "  model.fit(\n",
        "      X_train, y_train,\n",
        "      eval_set=(X_test, y_test),\n",
        "      verbose=100\n",
        "  );     \n",
        "      \n",
        "gpu_time = timeit.timeit('train_on_gpu()', \n",
        "                         setup=\"from __main__ import train_on_gpu\", \n",
        "                         number=1)\n",
        "\n",
        "print('Time to fit model on GPU: {} sec'.format(int(gpu_time)))\n",
        "print('GPU speedup over CPU: ' + '%.2f' % (cpu_time/gpu_time) + 'x')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0:\tlearn: 0.6419186\ttest: 0.6419137\tbest: 0.6419137 (0)\ttotal: 46ms\tremaining: 45.9s\n",
            "100:\tlearn: 0.1360534\ttest: 0.1385616\tbest: 0.1385616 (100)\ttotal: 1.25s\tremaining: 11.1s\n",
            "200:\tlearn: 0.1318218\ttest: 0.1362891\tbest: 0.1362869 (199)\ttotal: 2.32s\tremaining: 9.22s\n",
            "300:\tlearn: 0.1297227\ttest: 0.1357183\tbest: 0.1357183 (300)\ttotal: 3.38s\tremaining: 7.84s\n",
            "400:\tlearn: 0.1282696\ttest: 0.1354960\tbest: 0.1354960 (400)\ttotal: 4.44s\tremaining: 6.63s\n",
            "500:\tlearn: 0.1267707\ttest: 0.1354005\tbest: 0.1353966 (494)\ttotal: 5.51s\tremaining: 5.49s\n",
            "600:\tlearn: 0.1253700\ttest: 0.1352449\tbest: 0.1352449 (600)\ttotal: 6.62s\tremaining: 4.39s\n",
            "700:\tlearn: 0.1239492\ttest: 0.1352796\tbest: 0.1352240 (625)\ttotal: 7.78s\tremaining: 3.32s\n",
            "800:\tlearn: 0.1228055\ttest: 0.1352042\tbest: 0.1351763 (784)\ttotal: 8.91s\tremaining: 2.21s\n",
            "900:\tlearn: 0.1215164\ttest: 0.1351012\tbest: 0.1351012 (900)\ttotal: 10s\tremaining: 1.1s\n",
            "999:\tlearn: 0.1203040\ttest: 0.1350780\tbest: 0.1350476 (973)\ttotal: 11.1s\tremaining: 0us\n",
            "bestTest = 0.1350476135\n",
            "bestIteration = 973\n",
            "Shrink model to first 974 iterations.\n",
            "Time to fit model on GPU: 17 sec\n",
            "GPU speedup over CPU: 6.39x\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R38s_6TmpoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12Dew9HSnBfQ",
        "colab_type": "text"
      },
      "source": [
        "## 3.2 LGBM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-myYSYZm3Ud",
        "colab_type": "text"
      },
      "source": [
        "### Train in LGBM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6SAamFrmpv7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import lightgbm as lgb\n",
        "lgb_train = lgb.Dataset(X_train, y_train)\n",
        "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
        "# specify your configurations as a dict\n",
        "params = {\n",
        "    'task': 'train',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'binary',\n",
        "    'metric': {'binary_logloss'},\n",
        "    'learning_rate': 0.03\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAHwaXUqnKvw",
        "colab_type": "code",
        "outputId": "e6fbdb6c-3d90-4111-955e-d0bab5c1c059",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Use time function to measure time elapsed\n",
        "import time\n",
        "start = time.time()\n",
        "\n",
        "\n",
        "gbm = lgb.train(params,\n",
        "                lgb_train,\n",
        "                num_boost_round=1000,\n",
        "                valid_sets=lgb_train)\n",
        "\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1]\ttraining's binary_logloss: 0.163677\n",
            "[2]\ttraining's binary_logloss: 0.1612\n",
            "[3]\ttraining's binary_logloss: 0.159019\n",
            "[4]\ttraining's binary_logloss: 0.157068\n",
            "[5]\ttraining's binary_logloss: 0.155332\n",
            "[6]\ttraining's binary_logloss: 0.153751\n",
            "[7]\ttraining's binary_logloss: 0.152293\n",
            "[8]\ttraining's binary_logloss: 0.150965\n",
            "[9]\ttraining's binary_logloss: 0.149755\n",
            "[10]\ttraining's binary_logloss: 0.148618\n",
            "[11]\ttraining's binary_logloss: 0.147578\n",
            "[12]\ttraining's binary_logloss: 0.146561\n",
            "[13]\ttraining's binary_logloss: 0.145607\n",
            "[14]\ttraining's binary_logloss: 0.144724\n",
            "[15]\ttraining's binary_logloss: 0.143862\n",
            "[16]\ttraining's binary_logloss: 0.143089\n",
            "[17]\ttraining's binary_logloss: 0.14235\n",
            "[18]\ttraining's binary_logloss: 0.141639\n",
            "[19]\ttraining's binary_logloss: 0.140965\n",
            "[20]\ttraining's binary_logloss: 0.140324\n",
            "[21]\ttraining's binary_logloss: 0.139712\n",
            "[22]\ttraining's binary_logloss: 0.139141\n",
            "[23]\ttraining's binary_logloss: 0.138581\n",
            "[24]\ttraining's binary_logloss: 0.138023\n",
            "[25]\ttraining's binary_logloss: 0.13752\n",
            "[26]\ttraining's binary_logloss: 0.137019\n",
            "[27]\ttraining's binary_logloss: 0.13656\n",
            "[28]\ttraining's binary_logloss: 0.136104\n",
            "[29]\ttraining's binary_logloss: 0.135665\n",
            "[30]\ttraining's binary_logloss: 0.135241\n",
            "[31]\ttraining's binary_logloss: 0.134838\n",
            "[32]\ttraining's binary_logloss: 0.134464\n",
            "[33]\ttraining's binary_logloss: 0.134103\n",
            "[34]\ttraining's binary_logloss: 0.133737\n",
            "[35]\ttraining's binary_logloss: 0.13337\n",
            "[36]\ttraining's binary_logloss: 0.133054\n",
            "[37]\ttraining's binary_logloss: 0.132725\n",
            "[38]\ttraining's binary_logloss: 0.13241\n",
            "[39]\ttraining's binary_logloss: 0.13211\n",
            "[40]\ttraining's binary_logloss: 0.13181\n",
            "[41]\ttraining's binary_logloss: 0.131522\n",
            "[42]\ttraining's binary_logloss: 0.131228\n",
            "[43]\ttraining's binary_logloss: 0.130965\n",
            "[44]\ttraining's binary_logloss: 0.130702\n",
            "[45]\ttraining's binary_logloss: 0.130431\n",
            "[46]\ttraining's binary_logloss: 0.130166\n",
            "[47]\ttraining's binary_logloss: 0.129918\n",
            "[48]\ttraining's binary_logloss: 0.129671\n",
            "[49]\ttraining's binary_logloss: 0.129453\n",
            "[50]\ttraining's binary_logloss: 0.129245\n",
            "[51]\ttraining's binary_logloss: 0.129002\n",
            "[52]\ttraining's binary_logloss: 0.128799\n",
            "[53]\ttraining's binary_logloss: 0.128588\n",
            "[54]\ttraining's binary_logloss: 0.128384\n",
            "[55]\ttraining's binary_logloss: 0.128176\n",
            "[56]\ttraining's binary_logloss: 0.127988\n",
            "[57]\ttraining's binary_logloss: 0.127809\n",
            "[58]\ttraining's binary_logloss: 0.127586\n",
            "[59]\ttraining's binary_logloss: 0.127408\n",
            "[60]\ttraining's binary_logloss: 0.127228\n",
            "[61]\ttraining's binary_logloss: 0.127046\n",
            "[62]\ttraining's binary_logloss: 0.126867\n",
            "[63]\ttraining's binary_logloss: 0.126667\n",
            "[64]\ttraining's binary_logloss: 0.12648\n",
            "[65]\ttraining's binary_logloss: 0.12631\n",
            "[66]\ttraining's binary_logloss: 0.126152\n",
            "[67]\ttraining's binary_logloss: 0.126005\n",
            "[68]\ttraining's binary_logloss: 0.125837\n",
            "[69]\ttraining's binary_logloss: 0.125669\n",
            "[70]\ttraining's binary_logloss: 0.125526\n",
            "[71]\ttraining's binary_logloss: 0.125376\n",
            "[72]\ttraining's binary_logloss: 0.125233\n",
            "[73]\ttraining's binary_logloss: 0.125079\n",
            "[74]\ttraining's binary_logloss: 0.124939\n",
            "[75]\ttraining's binary_logloss: 0.124789\n",
            "[76]\ttraining's binary_logloss: 0.124647\n",
            "[77]\ttraining's binary_logloss: 0.124496\n",
            "[78]\ttraining's binary_logloss: 0.124363\n",
            "[79]\ttraining's binary_logloss: 0.124231\n",
            "[80]\ttraining's binary_logloss: 0.124096\n",
            "[81]\ttraining's binary_logloss: 0.123963\n",
            "[82]\ttraining's binary_logloss: 0.123833\n",
            "[83]\ttraining's binary_logloss: 0.123701\n",
            "[84]\ttraining's binary_logloss: 0.123568\n",
            "[85]\ttraining's binary_logloss: 0.123436\n",
            "[86]\ttraining's binary_logloss: 0.123317\n",
            "[87]\ttraining's binary_logloss: 0.1232\n",
            "[88]\ttraining's binary_logloss: 0.123088\n",
            "[89]\ttraining's binary_logloss: 0.122992\n",
            "[90]\ttraining's binary_logloss: 0.122866\n",
            "[91]\ttraining's binary_logloss: 0.122758\n",
            "[92]\ttraining's binary_logloss: 0.122597\n",
            "[93]\ttraining's binary_logloss: 0.122501\n",
            "[94]\ttraining's binary_logloss: 0.122378\n",
            "[95]\ttraining's binary_logloss: 0.122247\n",
            "[96]\ttraining's binary_logloss: 0.12215\n",
            "[97]\ttraining's binary_logloss: 0.122046\n",
            "[98]\ttraining's binary_logloss: 0.121944\n",
            "[99]\ttraining's binary_logloss: 0.121836\n",
            "[100]\ttraining's binary_logloss: 0.121683\n",
            "[101]\ttraining's binary_logloss: 0.121593\n",
            "[102]\ttraining's binary_logloss: 0.12149\n",
            "[103]\ttraining's binary_logloss: 0.121399\n",
            "[104]\ttraining's binary_logloss: 0.121298\n",
            "[105]\ttraining's binary_logloss: 0.121205\n",
            "[106]\ttraining's binary_logloss: 0.121106\n",
            "[107]\ttraining's binary_logloss: 0.121012\n",
            "[108]\ttraining's binary_logloss: 0.120892\n",
            "[109]\ttraining's binary_logloss: 0.120802\n",
            "[110]\ttraining's binary_logloss: 0.120698\n",
            "[111]\ttraining's binary_logloss: 0.120576\n",
            "[112]\ttraining's binary_logloss: 0.120465\n",
            "[113]\ttraining's binary_logloss: 0.120372\n",
            "[114]\ttraining's binary_logloss: 0.120283\n",
            "[115]\ttraining's binary_logloss: 0.120194\n",
            "[116]\ttraining's binary_logloss: 0.120101\n",
            "[117]\ttraining's binary_logloss: 0.119998\n",
            "[118]\ttraining's binary_logloss: 0.119902\n",
            "[119]\ttraining's binary_logloss: 0.119788\n",
            "[120]\ttraining's binary_logloss: 0.119689\n",
            "[121]\ttraining's binary_logloss: 0.119587\n",
            "[122]\ttraining's binary_logloss: 0.119493\n",
            "[123]\ttraining's binary_logloss: 0.119396\n",
            "[124]\ttraining's binary_logloss: 0.119287\n",
            "[125]\ttraining's binary_logloss: 0.119194\n",
            "[126]\ttraining's binary_logloss: 0.119099\n",
            "[127]\ttraining's binary_logloss: 0.118998\n",
            "[128]\ttraining's binary_logloss: 0.118908\n",
            "[129]\ttraining's binary_logloss: 0.118825\n",
            "[130]\ttraining's binary_logloss: 0.118708\n",
            "[131]\ttraining's binary_logloss: 0.11861\n",
            "[132]\ttraining's binary_logloss: 0.118541\n",
            "[133]\ttraining's binary_logloss: 0.118448\n",
            "[134]\ttraining's binary_logloss: 0.118342\n",
            "[135]\ttraining's binary_logloss: 0.118241\n",
            "[136]\ttraining's binary_logloss: 0.118154\n",
            "[137]\ttraining's binary_logloss: 0.118067\n",
            "[138]\ttraining's binary_logloss: 0.117983\n",
            "[139]\ttraining's binary_logloss: 0.117878\n",
            "[140]\ttraining's binary_logloss: 0.117784\n",
            "[141]\ttraining's binary_logloss: 0.11769\n",
            "[142]\ttraining's binary_logloss: 0.117596\n",
            "[143]\ttraining's binary_logloss: 0.11751\n",
            "[144]\ttraining's binary_logloss: 0.11743\n",
            "[145]\ttraining's binary_logloss: 0.117332\n",
            "[146]\ttraining's binary_logloss: 0.117252\n",
            "[147]\ttraining's binary_logloss: 0.117163\n",
            "[148]\ttraining's binary_logloss: 0.117089\n",
            "[149]\ttraining's binary_logloss: 0.117015\n",
            "[150]\ttraining's binary_logloss: 0.116948\n",
            "[151]\ttraining's binary_logloss: 0.116869\n",
            "[152]\ttraining's binary_logloss: 0.116785\n",
            "[153]\ttraining's binary_logloss: 0.116713\n",
            "[154]\ttraining's binary_logloss: 0.11662\n",
            "[155]\ttraining's binary_logloss: 0.116534\n",
            "[156]\ttraining's binary_logloss: 0.116462\n",
            "[157]\ttraining's binary_logloss: 0.11639\n",
            "[158]\ttraining's binary_logloss: 0.11631\n",
            "[159]\ttraining's binary_logloss: 0.116238\n",
            "[160]\ttraining's binary_logloss: 0.116152\n",
            "[161]\ttraining's binary_logloss: 0.116073\n",
            "[162]\ttraining's binary_logloss: 0.115998\n",
            "[163]\ttraining's binary_logloss: 0.115924\n",
            "[164]\ttraining's binary_logloss: 0.115858\n",
            "[165]\ttraining's binary_logloss: 0.115795\n",
            "[166]\ttraining's binary_logloss: 0.11571\n",
            "[167]\ttraining's binary_logloss: 0.11562\n",
            "[168]\ttraining's binary_logloss: 0.115536\n",
            "[169]\ttraining's binary_logloss: 0.115459\n",
            "[170]\ttraining's binary_logloss: 0.115365\n",
            "[171]\ttraining's binary_logloss: 0.115281\n",
            "[172]\ttraining's binary_logloss: 0.115185\n",
            "[173]\ttraining's binary_logloss: 0.115077\n",
            "[174]\ttraining's binary_logloss: 0.114999\n",
            "[175]\ttraining's binary_logloss: 0.114933\n",
            "[176]\ttraining's binary_logloss: 0.114858\n",
            "[177]\ttraining's binary_logloss: 0.114785\n",
            "[178]\ttraining's binary_logloss: 0.114716\n",
            "[179]\ttraining's binary_logloss: 0.114632\n",
            "[180]\ttraining's binary_logloss: 0.114534\n",
            "[181]\ttraining's binary_logloss: 0.114467\n",
            "[182]\ttraining's binary_logloss: 0.114354\n",
            "[183]\ttraining's binary_logloss: 0.11428\n",
            "[184]\ttraining's binary_logloss: 0.114217\n",
            "[185]\ttraining's binary_logloss: 0.114154\n",
            "[186]\ttraining's binary_logloss: 0.114093\n",
            "[187]\ttraining's binary_logloss: 0.11401\n",
            "[188]\ttraining's binary_logloss: 0.113952\n",
            "[189]\ttraining's binary_logloss: 0.1139\n",
            "[190]\ttraining's binary_logloss: 0.113825\n",
            "[191]\ttraining's binary_logloss: 0.113741\n",
            "[192]\ttraining's binary_logloss: 0.113678\n",
            "[193]\ttraining's binary_logloss: 0.113609\n",
            "[194]\ttraining's binary_logloss: 0.11356\n",
            "[195]\ttraining's binary_logloss: 0.113498\n",
            "[196]\ttraining's binary_logloss: 0.113417\n",
            "[197]\ttraining's binary_logloss: 0.113344\n",
            "[198]\ttraining's binary_logloss: 0.113276\n",
            "[199]\ttraining's binary_logloss: 0.11322\n",
            "[200]\ttraining's binary_logloss: 0.113154\n",
            "[201]\ttraining's binary_logloss: 0.113094\n",
            "[202]\ttraining's binary_logloss: 0.113042\n",
            "[203]\ttraining's binary_logloss: 0.112961\n",
            "[204]\ttraining's binary_logloss: 0.112894\n",
            "[205]\ttraining's binary_logloss: 0.112846\n",
            "[206]\ttraining's binary_logloss: 0.112788\n",
            "[207]\ttraining's binary_logloss: 0.112719\n",
            "[208]\ttraining's binary_logloss: 0.112639\n",
            "[209]\ttraining's binary_logloss: 0.112574\n",
            "[210]\ttraining's binary_logloss: 0.112502\n",
            "[211]\ttraining's binary_logloss: 0.112436\n",
            "[212]\ttraining's binary_logloss: 0.112357\n",
            "[213]\ttraining's binary_logloss: 0.112307\n",
            "[214]\ttraining's binary_logloss: 0.112248\n",
            "[215]\ttraining's binary_logloss: 0.112199\n",
            "[216]\ttraining's binary_logloss: 0.112134\n",
            "[217]\ttraining's binary_logloss: 0.112072\n",
            "[218]\ttraining's binary_logloss: 0.112012\n",
            "[219]\ttraining's binary_logloss: 0.111903\n",
            "[220]\ttraining's binary_logloss: 0.111855\n",
            "[221]\ttraining's binary_logloss: 0.1118\n",
            "[222]\ttraining's binary_logloss: 0.111733\n",
            "[223]\ttraining's binary_logloss: 0.111657\n",
            "[224]\ttraining's binary_logloss: 0.111584\n",
            "[225]\ttraining's binary_logloss: 0.111515\n",
            "[226]\ttraining's binary_logloss: 0.111433\n",
            "[227]\ttraining's binary_logloss: 0.111372\n",
            "[228]\ttraining's binary_logloss: 0.111314\n",
            "[229]\ttraining's binary_logloss: 0.111265\n",
            "[230]\ttraining's binary_logloss: 0.111195\n",
            "[231]\ttraining's binary_logloss: 0.111152\n",
            "[232]\ttraining's binary_logloss: 0.111113\n",
            "[233]\ttraining's binary_logloss: 0.111066\n",
            "[234]\ttraining's binary_logloss: 0.110999\n",
            "[235]\ttraining's binary_logloss: 0.110951\n",
            "[236]\ttraining's binary_logloss: 0.11088\n",
            "[237]\ttraining's binary_logloss: 0.110806\n",
            "[238]\ttraining's binary_logloss: 0.110736\n",
            "[239]\ttraining's binary_logloss: 0.110664\n",
            "[240]\ttraining's binary_logloss: 0.110604\n",
            "[241]\ttraining's binary_logloss: 0.110559\n",
            "[242]\ttraining's binary_logloss: 0.110507\n",
            "[243]\ttraining's binary_logloss: 0.11047\n",
            "[244]\ttraining's binary_logloss: 0.110421\n",
            "[245]\ttraining's binary_logloss: 0.110354\n",
            "[246]\ttraining's binary_logloss: 0.110306\n",
            "[247]\ttraining's binary_logloss: 0.11027\n",
            "[248]\ttraining's binary_logloss: 0.110189\n",
            "[249]\ttraining's binary_logloss: 0.110152\n",
            "[250]\ttraining's binary_logloss: 0.110094\n",
            "[251]\ttraining's binary_logloss: 0.110056\n",
            "[252]\ttraining's binary_logloss: 0.109981\n",
            "[253]\ttraining's binary_logloss: 0.109892\n",
            "[254]\ttraining's binary_logloss: 0.109841\n",
            "[255]\ttraining's binary_logloss: 0.109811\n",
            "[256]\ttraining's binary_logloss: 0.109771\n",
            "[257]\ttraining's binary_logloss: 0.109717\n",
            "[258]\ttraining's binary_logloss: 0.109655\n",
            "[259]\ttraining's binary_logloss: 0.109581\n",
            "[260]\ttraining's binary_logloss: 0.109526\n",
            "[261]\ttraining's binary_logloss: 0.109456\n",
            "[262]\ttraining's binary_logloss: 0.109389\n",
            "[263]\ttraining's binary_logloss: 0.109353\n",
            "[264]\ttraining's binary_logloss: 0.109291\n",
            "[265]\ttraining's binary_logloss: 0.109237\n",
            "[266]\ttraining's binary_logloss: 0.109178\n",
            "[267]\ttraining's binary_logloss: 0.109128\n",
            "[268]\ttraining's binary_logloss: 0.109046\n",
            "[269]\ttraining's binary_logloss: 0.10898\n",
            "[270]\ttraining's binary_logloss: 0.108867\n",
            "[271]\ttraining's binary_logloss: 0.108826\n",
            "[272]\ttraining's binary_logloss: 0.108792\n",
            "[273]\ttraining's binary_logloss: 0.108742\n",
            "[274]\ttraining's binary_logloss: 0.108659\n",
            "[275]\ttraining's binary_logloss: 0.108611\n",
            "[276]\ttraining's binary_logloss: 0.108537\n",
            "[277]\ttraining's binary_logloss: 0.108478\n",
            "[278]\ttraining's binary_logloss: 0.108442\n",
            "[279]\ttraining's binary_logloss: 0.108389\n",
            "[280]\ttraining's binary_logloss: 0.108326\n",
            "[281]\ttraining's binary_logloss: 0.108256\n",
            "[282]\ttraining's binary_logloss: 0.108176\n",
            "[283]\ttraining's binary_logloss: 0.108107\n",
            "[284]\ttraining's binary_logloss: 0.108059\n",
            "[285]\ttraining's binary_logloss: 0.108018\n",
            "[286]\ttraining's binary_logloss: 0.107921\n",
            "[287]\ttraining's binary_logloss: 0.107861\n",
            "[288]\ttraining's binary_logloss: 0.107828\n",
            "[289]\ttraining's binary_logloss: 0.107777\n",
            "[290]\ttraining's binary_logloss: 0.107721\n",
            "[291]\ttraining's binary_logloss: 0.107694\n",
            "[292]\ttraining's binary_logloss: 0.107648\n",
            "[293]\ttraining's binary_logloss: 0.107584\n",
            "[294]\ttraining's binary_logloss: 0.107518\n",
            "[295]\ttraining's binary_logloss: 0.107442\n",
            "[296]\ttraining's binary_logloss: 0.107411\n",
            "[297]\ttraining's binary_logloss: 0.10736\n",
            "[298]\ttraining's binary_logloss: 0.107327\n",
            "[299]\ttraining's binary_logloss: 0.107229\n",
            "[300]\ttraining's binary_logloss: 0.107194\n",
            "[301]\ttraining's binary_logloss: 0.107158\n",
            "[302]\ttraining's binary_logloss: 0.107116\n",
            "[303]\ttraining's binary_logloss: 0.107047\n",
            "[304]\ttraining's binary_logloss: 0.107005\n",
            "[305]\ttraining's binary_logloss: 0.106932\n",
            "[306]\ttraining's binary_logloss: 0.106882\n",
            "[307]\ttraining's binary_logloss: 0.106833\n",
            "[308]\ttraining's binary_logloss: 0.106772\n",
            "[309]\ttraining's binary_logloss: 0.106713\n",
            "[310]\ttraining's binary_logloss: 0.106663\n",
            "[311]\ttraining's binary_logloss: 0.106624\n",
            "[312]\ttraining's binary_logloss: 0.10656\n",
            "[313]\ttraining's binary_logloss: 0.106511\n",
            "[314]\ttraining's binary_logloss: 0.106473\n",
            "[315]\ttraining's binary_logloss: 0.106426\n",
            "[316]\ttraining's binary_logloss: 0.106379\n",
            "[317]\ttraining's binary_logloss: 0.106339\n",
            "[318]\ttraining's binary_logloss: 0.106299\n",
            "[319]\ttraining's binary_logloss: 0.106237\n",
            "[320]\ttraining's binary_logloss: 0.106192\n",
            "[321]\ttraining's binary_logloss: 0.106136\n",
            "[322]\ttraining's binary_logloss: 0.106071\n",
            "[323]\ttraining's binary_logloss: 0.106045\n",
            "[324]\ttraining's binary_logloss: 0.106012\n",
            "[325]\ttraining's binary_logloss: 0.10593\n",
            "[326]\ttraining's binary_logloss: 0.105862\n",
            "[327]\ttraining's binary_logloss: 0.105819\n",
            "[328]\ttraining's binary_logloss: 0.10575\n",
            "[329]\ttraining's binary_logloss: 0.105695\n",
            "[330]\ttraining's binary_logloss: 0.10563\n",
            "[331]\ttraining's binary_logloss: 0.105581\n",
            "[332]\ttraining's binary_logloss: 0.105533\n",
            "[333]\ttraining's binary_logloss: 0.105487\n",
            "[334]\ttraining's binary_logloss: 0.105412\n",
            "[335]\ttraining's binary_logloss: 0.105378\n",
            "[336]\ttraining's binary_logloss: 0.105321\n",
            "[337]\ttraining's binary_logloss: 0.105285\n",
            "[338]\ttraining's binary_logloss: 0.105255\n",
            "[339]\ttraining's binary_logloss: 0.105194\n",
            "[340]\ttraining's binary_logloss: 0.105115\n",
            "[341]\ttraining's binary_logloss: 0.10506\n",
            "[342]\ttraining's binary_logloss: 0.104996\n",
            "[343]\ttraining's binary_logloss: 0.104923\n",
            "[344]\ttraining's binary_logloss: 0.104892\n",
            "[345]\ttraining's binary_logloss: 0.104841\n",
            "[346]\ttraining's binary_logloss: 0.104814\n",
            "[347]\ttraining's binary_logloss: 0.104729\n",
            "[348]\ttraining's binary_logloss: 0.104675\n",
            "[349]\ttraining's binary_logloss: 0.10462\n",
            "[350]\ttraining's binary_logloss: 0.104551\n",
            "[351]\ttraining's binary_logloss: 0.104488\n",
            "[352]\ttraining's binary_logloss: 0.104421\n",
            "[353]\ttraining's binary_logloss: 0.104391\n",
            "[354]\ttraining's binary_logloss: 0.104344\n",
            "[355]\ttraining's binary_logloss: 0.104275\n",
            "[356]\ttraining's binary_logloss: 0.104212\n",
            "[357]\ttraining's binary_logloss: 0.104177\n",
            "[358]\ttraining's binary_logloss: 0.104141\n",
            "[359]\ttraining's binary_logloss: 0.104056\n",
            "[360]\ttraining's binary_logloss: 0.104015\n",
            "[361]\ttraining's binary_logloss: 0.103966\n",
            "[362]\ttraining's binary_logloss: 0.103939\n",
            "[363]\ttraining's binary_logloss: 0.103909\n",
            "[364]\ttraining's binary_logloss: 0.103865\n",
            "[365]\ttraining's binary_logloss: 0.103835\n",
            "[366]\ttraining's binary_logloss: 0.103779\n",
            "[367]\ttraining's binary_logloss: 0.103756\n",
            "[368]\ttraining's binary_logloss: 0.103713\n",
            "[369]\ttraining's binary_logloss: 0.103664\n",
            "[370]\ttraining's binary_logloss: 0.103625\n",
            "[371]\ttraining's binary_logloss: 0.103601\n",
            "[372]\ttraining's binary_logloss: 0.103541\n",
            "[373]\ttraining's binary_logloss: 0.103446\n",
            "[374]\ttraining's binary_logloss: 0.103393\n",
            "[375]\ttraining's binary_logloss: 0.103311\n",
            "[376]\ttraining's binary_logloss: 0.103257\n",
            "[377]\ttraining's binary_logloss: 0.103228\n",
            "[378]\ttraining's binary_logloss: 0.103188\n",
            "[379]\ttraining's binary_logloss: 0.103127\n",
            "[380]\ttraining's binary_logloss: 0.103099\n",
            "[381]\ttraining's binary_logloss: 0.103027\n",
            "[382]\ttraining's binary_logloss: 0.10298\n",
            "[383]\ttraining's binary_logloss: 0.102934\n",
            "[384]\ttraining's binary_logloss: 0.102862\n",
            "[385]\ttraining's binary_logloss: 0.102835\n",
            "[386]\ttraining's binary_logloss: 0.102812\n",
            "[387]\ttraining's binary_logloss: 0.10279\n",
            "[388]\ttraining's binary_logloss: 0.102734\n",
            "[389]\ttraining's binary_logloss: 0.102709\n",
            "[390]\ttraining's binary_logloss: 0.102668\n",
            "[391]\ttraining's binary_logloss: 0.102609\n",
            "[392]\ttraining's binary_logloss: 0.102545\n",
            "[393]\ttraining's binary_logloss: 0.102482\n",
            "[394]\ttraining's binary_logloss: 0.102444\n",
            "[395]\ttraining's binary_logloss: 0.10239\n",
            "[396]\ttraining's binary_logloss: 0.102323\n",
            "[397]\ttraining's binary_logloss: 0.102299\n",
            "[398]\ttraining's binary_logloss: 0.102253\n",
            "[399]\ttraining's binary_logloss: 0.102199\n",
            "[400]\ttraining's binary_logloss: 0.102139\n",
            "[401]\ttraining's binary_logloss: 0.10209\n",
            "[402]\ttraining's binary_logloss: 0.102048\n",
            "[403]\ttraining's binary_logloss: 0.102013\n",
            "[404]\ttraining's binary_logloss: 0.10198\n",
            "[405]\ttraining's binary_logloss: 0.10195\n",
            "[406]\ttraining's binary_logloss: 0.101891\n",
            "[407]\ttraining's binary_logloss: 0.101868\n",
            "[408]\ttraining's binary_logloss: 0.101797\n",
            "[409]\ttraining's binary_logloss: 0.101753\n",
            "[410]\ttraining's binary_logloss: 0.101694\n",
            "[411]\ttraining's binary_logloss: 0.101663\n",
            "[412]\ttraining's binary_logloss: 0.101613\n",
            "[413]\ttraining's binary_logloss: 0.101582\n",
            "[414]\ttraining's binary_logloss: 0.101556\n",
            "[415]\ttraining's binary_logloss: 0.101508\n",
            "[416]\ttraining's binary_logloss: 0.101446\n",
            "[417]\ttraining's binary_logloss: 0.101399\n",
            "[418]\ttraining's binary_logloss: 0.101338\n",
            "[419]\ttraining's binary_logloss: 0.101289\n",
            "[420]\ttraining's binary_logloss: 0.101259\n",
            "[421]\ttraining's binary_logloss: 0.101224\n",
            "[422]\ttraining's binary_logloss: 0.101182\n",
            "[423]\ttraining's binary_logloss: 0.101137\n",
            "[424]\ttraining's binary_logloss: 0.10108\n",
            "[425]\ttraining's binary_logloss: 0.101007\n",
            "[426]\ttraining's binary_logloss: 0.100955\n",
            "[427]\ttraining's binary_logloss: 0.100891\n",
            "[428]\ttraining's binary_logloss: 0.100857\n",
            "[429]\ttraining's binary_logloss: 0.100811\n",
            "[430]\ttraining's binary_logloss: 0.100762\n",
            "[431]\ttraining's binary_logloss: 0.100707\n",
            "[432]\ttraining's binary_logloss: 0.100658\n",
            "[433]\ttraining's binary_logloss: 0.100626\n",
            "[434]\ttraining's binary_logloss: 0.100605\n",
            "[435]\ttraining's binary_logloss: 0.100574\n",
            "[436]\ttraining's binary_logloss: 0.100531\n",
            "[437]\ttraining's binary_logloss: 0.10051\n",
            "[438]\ttraining's binary_logloss: 0.100489\n",
            "[439]\ttraining's binary_logloss: 0.100436\n",
            "[440]\ttraining's binary_logloss: 0.100381\n",
            "[441]\ttraining's binary_logloss: 0.10031\n",
            "[442]\ttraining's binary_logloss: 0.10029\n",
            "[443]\ttraining's binary_logloss: 0.10026\n",
            "[444]\ttraining's binary_logloss: 0.100218\n",
            "[445]\ttraining's binary_logloss: 0.100135\n",
            "[446]\ttraining's binary_logloss: 0.100087\n",
            "[447]\ttraining's binary_logloss: 0.100061\n",
            "[448]\ttraining's binary_logloss: 0.100014\n",
            "[449]\ttraining's binary_logloss: 0.0999592\n",
            "[450]\ttraining's binary_logloss: 0.0998922\n",
            "[451]\ttraining's binary_logloss: 0.0998711\n",
            "[452]\ttraining's binary_logloss: 0.0998037\n",
            "[453]\ttraining's binary_logloss: 0.099735\n",
            "[454]\ttraining's binary_logloss: 0.0996727\n",
            "[455]\ttraining's binary_logloss: 0.0996171\n",
            "[456]\ttraining's binary_logloss: 0.0995853\n",
            "[457]\ttraining's binary_logloss: 0.0995408\n",
            "[458]\ttraining's binary_logloss: 0.0995075\n",
            "[459]\ttraining's binary_logloss: 0.0994792\n",
            "[460]\ttraining's binary_logloss: 0.0994508\n",
            "[461]\ttraining's binary_logloss: 0.0993939\n",
            "[462]\ttraining's binary_logloss: 0.0993741\n",
            "[463]\ttraining's binary_logloss: 0.0993338\n",
            "[464]\ttraining's binary_logloss: 0.0992628\n",
            "[465]\ttraining's binary_logloss: 0.0992214\n",
            "[466]\ttraining's binary_logloss: 0.0991952\n",
            "[467]\ttraining's binary_logloss: 0.0991691\n",
            "[468]\ttraining's binary_logloss: 0.0991366\n",
            "[469]\ttraining's binary_logloss: 0.0990845\n",
            "[470]\ttraining's binary_logloss: 0.0990578\n",
            "[471]\ttraining's binary_logloss: 0.0989873\n",
            "[472]\ttraining's binary_logloss: 0.0989589\n",
            "[473]\ttraining's binary_logloss: 0.0989247\n",
            "[474]\ttraining's binary_logloss: 0.0988859\n",
            "[475]\ttraining's binary_logloss: 0.0988694\n",
            "[476]\ttraining's binary_logloss: 0.0988263\n",
            "[477]\ttraining's binary_logloss: 0.0987811\n",
            "[478]\ttraining's binary_logloss: 0.0987475\n",
            "[479]\ttraining's binary_logloss: 0.0986465\n",
            "[480]\ttraining's binary_logloss: 0.09861\n",
            "[481]\ttraining's binary_logloss: 0.098582\n",
            "[482]\ttraining's binary_logloss: 0.098548\n",
            "[483]\ttraining's binary_logloss: 0.0984848\n",
            "[484]\ttraining's binary_logloss: 0.0984442\n",
            "[485]\ttraining's binary_logloss: 0.0984156\n",
            "[486]\ttraining's binary_logloss: 0.0983658\n",
            "[487]\ttraining's binary_logloss: 0.0983397\n",
            "[488]\ttraining's binary_logloss: 0.098286\n",
            "[489]\ttraining's binary_logloss: 0.0982492\n",
            "[490]\ttraining's binary_logloss: 0.0981945\n",
            "[491]\ttraining's binary_logloss: 0.0981742\n",
            "[492]\ttraining's binary_logloss: 0.0981528\n",
            "[493]\ttraining's binary_logloss: 0.0980637\n",
            "[494]\ttraining's binary_logloss: 0.0980229\n",
            "[495]\ttraining's binary_logloss: 0.0979624\n",
            "[496]\ttraining's binary_logloss: 0.097929\n",
            "[497]\ttraining's binary_logloss: 0.0979016\n",
            "[498]\ttraining's binary_logloss: 0.0978563\n",
            "[499]\ttraining's binary_logloss: 0.0978366\n",
            "[500]\ttraining's binary_logloss: 0.0978022\n",
            "[501]\ttraining's binary_logloss: 0.0977812\n",
            "[502]\ttraining's binary_logloss: 0.0977221\n",
            "[503]\ttraining's binary_logloss: 0.0977006\n",
            "[504]\ttraining's binary_logloss: 0.0976684\n",
            "[505]\ttraining's binary_logloss: 0.0976202\n",
            "[506]\ttraining's binary_logloss: 0.0975828\n",
            "[507]\ttraining's binary_logloss: 0.0975207\n",
            "[508]\ttraining's binary_logloss: 0.0974828\n",
            "[509]\ttraining's binary_logloss: 0.0974434\n",
            "[510]\ttraining's binary_logloss: 0.0973781\n",
            "[511]\ttraining's binary_logloss: 0.0973565\n",
            "[512]\ttraining's binary_logloss: 0.097332\n",
            "[513]\ttraining's binary_logloss: 0.0973091\n",
            "[514]\ttraining's binary_logloss: 0.097256\n",
            "[515]\ttraining's binary_logloss: 0.0972225\n",
            "[516]\ttraining's binary_logloss: 0.0971876\n",
            "[517]\ttraining's binary_logloss: 0.0971682\n",
            "[518]\ttraining's binary_logloss: 0.0971452\n",
            "[519]\ttraining's binary_logloss: 0.0971207\n",
            "[520]\ttraining's binary_logloss: 0.0970991\n",
            "[521]\ttraining's binary_logloss: 0.0970447\n",
            "[522]\ttraining's binary_logloss: 0.0970118\n",
            "[523]\ttraining's binary_logloss: 0.0969641\n",
            "[524]\ttraining's binary_logloss: 0.0969352\n",
            "[525]\ttraining's binary_logloss: 0.0968814\n",
            "[526]\ttraining's binary_logloss: 0.0968575\n",
            "[527]\ttraining's binary_logloss: 0.0968286\n",
            "[528]\ttraining's binary_logloss: 0.0968045\n",
            "[529]\ttraining's binary_logloss: 0.0967842\n",
            "[530]\ttraining's binary_logloss: 0.0967636\n",
            "[531]\ttraining's binary_logloss: 0.0967018\n",
            "[532]\ttraining's binary_logloss: 0.0966639\n",
            "[533]\ttraining's binary_logloss: 0.0966407\n",
            "[534]\ttraining's binary_logloss: 0.0966045\n",
            "[535]\ttraining's binary_logloss: 0.0965477\n",
            "[536]\ttraining's binary_logloss: 0.0965301\n",
            "[537]\ttraining's binary_logloss: 0.0965071\n",
            "[538]\ttraining's binary_logloss: 0.0964462\n",
            "[539]\ttraining's binary_logloss: 0.0963899\n",
            "[540]\ttraining's binary_logloss: 0.0963357\n",
            "[541]\ttraining's binary_logloss: 0.0962732\n",
            "[542]\ttraining's binary_logloss: 0.0962564\n",
            "[543]\ttraining's binary_logloss: 0.0962276\n",
            "[544]\ttraining's binary_logloss: 0.0961638\n",
            "[545]\ttraining's binary_logloss: 0.0961131\n",
            "[546]\ttraining's binary_logloss: 0.0960979\n",
            "[547]\ttraining's binary_logloss: 0.0960743\n",
            "[548]\ttraining's binary_logloss: 0.0960477\n",
            "[549]\ttraining's binary_logloss: 0.0959732\n",
            "[550]\ttraining's binary_logloss: 0.0959283\n",
            "[551]\ttraining's binary_logloss: 0.0959062\n",
            "[552]\ttraining's binary_logloss: 0.095857\n",
            "[553]\ttraining's binary_logloss: 0.0958141\n",
            "[554]\ttraining's binary_logloss: 0.095781\n",
            "[555]\ttraining's binary_logloss: 0.0957553\n",
            "[556]\ttraining's binary_logloss: 0.0957287\n",
            "[557]\ttraining's binary_logloss: 0.0956744\n",
            "[558]\ttraining's binary_logloss: 0.0956232\n",
            "[559]\ttraining's binary_logloss: 0.0956077\n",
            "[560]\ttraining's binary_logloss: 0.0955622\n",
            "[561]\ttraining's binary_logloss: 0.0955494\n",
            "[562]\ttraining's binary_logloss: 0.0955222\n",
            "[563]\ttraining's binary_logloss: 0.0954939\n",
            "[564]\ttraining's binary_logloss: 0.0954416\n",
            "[565]\ttraining's binary_logloss: 0.0953929\n",
            "[566]\ttraining's binary_logloss: 0.0953208\n",
            "[567]\ttraining's binary_logloss: 0.0952949\n",
            "[568]\ttraining's binary_logloss: 0.0952703\n",
            "[569]\ttraining's binary_logloss: 0.0952503\n",
            "[570]\ttraining's binary_logloss: 0.0952047\n",
            "[571]\ttraining's binary_logloss: 0.0951633\n",
            "[572]\ttraining's binary_logloss: 0.0951145\n",
            "[573]\ttraining's binary_logloss: 0.0950586\n",
            "[574]\ttraining's binary_logloss: 0.0949888\n",
            "[575]\ttraining's binary_logloss: 0.0949723\n",
            "[576]\ttraining's binary_logloss: 0.0949571\n",
            "[577]\ttraining's binary_logloss: 0.0949336\n",
            "[578]\ttraining's binary_logloss: 0.0949128\n",
            "[579]\ttraining's binary_logloss: 0.0948813\n",
            "[580]\ttraining's binary_logloss: 0.0948544\n",
            "[581]\ttraining's binary_logloss: 0.0948297\n",
            "[582]\ttraining's binary_logloss: 0.0948144\n",
            "[583]\ttraining's binary_logloss: 0.0947993\n",
            "[584]\ttraining's binary_logloss: 0.0947657\n",
            "[585]\ttraining's binary_logloss: 0.094713\n",
            "[586]\ttraining's binary_logloss: 0.09467\n",
            "[587]\ttraining's binary_logloss: 0.0945943\n",
            "[588]\ttraining's binary_logloss: 0.0945395\n",
            "[589]\ttraining's binary_logloss: 0.094515\n",
            "[590]\ttraining's binary_logloss: 0.094454\n",
            "[591]\ttraining's binary_logloss: 0.0943872\n",
            "[592]\ttraining's binary_logloss: 0.0943301\n",
            "[593]\ttraining's binary_logloss: 0.0942834\n",
            "[594]\ttraining's binary_logloss: 0.0942335\n",
            "[595]\ttraining's binary_logloss: 0.0942187\n",
            "[596]\ttraining's binary_logloss: 0.0941687\n",
            "[597]\ttraining's binary_logloss: 0.0941254\n",
            "[598]\ttraining's binary_logloss: 0.0941045\n",
            "[599]\ttraining's binary_logloss: 0.0940515\n",
            "[600]\ttraining's binary_logloss: 0.0939874\n",
            "[601]\ttraining's binary_logloss: 0.0939367\n",
            "[602]\ttraining's binary_logloss: 0.0938869\n",
            "[603]\ttraining's binary_logloss: 0.0938638\n",
            "[604]\ttraining's binary_logloss: 0.0937976\n",
            "[605]\ttraining's binary_logloss: 0.0937466\n",
            "[606]\ttraining's binary_logloss: 0.0936963\n",
            "[607]\ttraining's binary_logloss: 0.093613\n",
            "[608]\ttraining's binary_logloss: 0.0935792\n",
            "[609]\ttraining's binary_logloss: 0.0935305\n",
            "[610]\ttraining's binary_logloss: 0.0934836\n",
            "[611]\ttraining's binary_logloss: 0.093454\n",
            "[612]\ttraining's binary_logloss: 0.0934339\n",
            "[613]\ttraining's binary_logloss: 0.0933878\n",
            "[614]\ttraining's binary_logloss: 0.0933685\n",
            "[615]\ttraining's binary_logloss: 0.0933103\n",
            "[616]\ttraining's binary_logloss: 0.0932684\n",
            "[617]\ttraining's binary_logloss: 0.0932316\n",
            "[618]\ttraining's binary_logloss: 0.0931936\n",
            "[619]\ttraining's binary_logloss: 0.093155\n",
            "[620]\ttraining's binary_logloss: 0.0931005\n",
            "[621]\ttraining's binary_logloss: 0.0930495\n",
            "[622]\ttraining's binary_logloss: 0.0930328\n",
            "[623]\ttraining's binary_logloss: 0.092987\n",
            "[624]\ttraining's binary_logloss: 0.0929729\n",
            "[625]\ttraining's binary_logloss: 0.0929587\n",
            "[626]\ttraining's binary_logloss: 0.0929331\n",
            "[627]\ttraining's binary_logloss: 0.0929176\n",
            "[628]\ttraining's binary_logloss: 0.0928724\n",
            "[629]\ttraining's binary_logloss: 0.0928171\n",
            "[630]\ttraining's binary_logloss: 0.0927679\n",
            "[631]\ttraining's binary_logloss: 0.0927498\n",
            "[632]\ttraining's binary_logloss: 0.0927154\n",
            "[633]\ttraining's binary_logloss: 0.0926747\n",
            "[634]\ttraining's binary_logloss: 0.0926312\n",
            "[635]\ttraining's binary_logloss: 0.0926084\n",
            "[636]\ttraining's binary_logloss: 0.0925529\n",
            "[637]\ttraining's binary_logloss: 0.0924897\n",
            "[638]\ttraining's binary_logloss: 0.0924587\n",
            "[639]\ttraining's binary_logloss: 0.092417\n",
            "[640]\ttraining's binary_logloss: 0.0923866\n",
            "[641]\ttraining's binary_logloss: 0.0923415\n",
            "[642]\ttraining's binary_logloss: 0.0922924\n",
            "[643]\ttraining's binary_logloss: 0.0922374\n",
            "[644]\ttraining's binary_logloss: 0.0922234\n",
            "[645]\ttraining's binary_logloss: 0.0922082\n",
            "[646]\ttraining's binary_logloss: 0.0921698\n",
            "[647]\ttraining's binary_logloss: 0.0921187\n",
            "[648]\ttraining's binary_logloss: 0.0920905\n",
            "[649]\ttraining's binary_logloss: 0.0920484\n",
            "[650]\ttraining's binary_logloss: 0.0920361\n",
            "[651]\ttraining's binary_logloss: 0.0919851\n",
            "[652]\ttraining's binary_logloss: 0.0919427\n",
            "[653]\ttraining's binary_logloss: 0.0919264\n",
            "[654]\ttraining's binary_logloss: 0.0918897\n",
            "[655]\ttraining's binary_logloss: 0.091861\n",
            "[656]\ttraining's binary_logloss: 0.0917993\n",
            "[657]\ttraining's binary_logloss: 0.0917831\n",
            "[658]\ttraining's binary_logloss: 0.0917478\n",
            "[659]\ttraining's binary_logloss: 0.0917291\n",
            "[660]\ttraining's binary_logloss: 0.0916648\n",
            "[661]\ttraining's binary_logloss: 0.0916282\n",
            "[662]\ttraining's binary_logloss: 0.0915976\n",
            "[663]\ttraining's binary_logloss: 0.0915552\n",
            "[664]\ttraining's binary_logloss: 0.0915346\n",
            "[665]\ttraining's binary_logloss: 0.0915212\n",
            "[666]\ttraining's binary_logloss: 0.0915008\n",
            "[667]\ttraining's binary_logloss: 0.0914435\n",
            "[668]\ttraining's binary_logloss: 0.0914166\n",
            "[669]\ttraining's binary_logloss: 0.0913843\n",
            "[670]\ttraining's binary_logloss: 0.0913645\n",
            "[671]\ttraining's binary_logloss: 0.0913159\n",
            "[672]\ttraining's binary_logloss: 0.091283\n",
            "[673]\ttraining's binary_logloss: 0.0912352\n",
            "[674]\ttraining's binary_logloss: 0.0911875\n",
            "[675]\ttraining's binary_logloss: 0.0911743\n",
            "[676]\ttraining's binary_logloss: 0.0911572\n",
            "[677]\ttraining's binary_logloss: 0.091121\n",
            "[678]\ttraining's binary_logloss: 0.0910712\n",
            "[679]\ttraining's binary_logloss: 0.0910453\n",
            "[680]\ttraining's binary_logloss: 0.0910057\n",
            "[681]\ttraining's binary_logloss: 0.0909927\n",
            "[682]\ttraining's binary_logloss: 0.0909412\n",
            "[683]\ttraining's binary_logloss: 0.0908891\n",
            "[684]\ttraining's binary_logloss: 0.0908423\n",
            "[685]\ttraining's binary_logloss: 0.0907836\n",
            "[686]\ttraining's binary_logloss: 0.0907711\n",
            "[687]\ttraining's binary_logloss: 0.0907593\n",
            "[688]\ttraining's binary_logloss: 0.0907439\n",
            "[689]\ttraining's binary_logloss: 0.09072\n",
            "[690]\ttraining's binary_logloss: 0.0906725\n",
            "[691]\ttraining's binary_logloss: 0.0906342\n",
            "[692]\ttraining's binary_logloss: 0.0906044\n",
            "[693]\ttraining's binary_logloss: 0.0905448\n",
            "[694]\ttraining's binary_logloss: 0.0905119\n",
            "[695]\ttraining's binary_logloss: 0.0904663\n",
            "[696]\ttraining's binary_logloss: 0.0904261\n",
            "[697]\ttraining's binary_logloss: 0.0903712\n",
            "[698]\ttraining's binary_logloss: 0.0903217\n",
            "[699]\ttraining's binary_logloss: 0.0902739\n",
            "[700]\ttraining's binary_logloss: 0.0902237\n",
            "[701]\ttraining's binary_logloss: 0.090191\n",
            "[702]\ttraining's binary_logloss: 0.090163\n",
            "[703]\ttraining's binary_logloss: 0.0901351\n",
            "[704]\ttraining's binary_logloss: 0.0900878\n",
            "[705]\ttraining's binary_logloss: 0.0900472\n",
            "[706]\ttraining's binary_logloss: 0.0900015\n",
            "[707]\ttraining's binary_logloss: 0.0899617\n",
            "[708]\ttraining's binary_logloss: 0.0899418\n",
            "[709]\ttraining's binary_logloss: 0.0899123\n",
            "[710]\ttraining's binary_logloss: 0.0899006\n",
            "[711]\ttraining's binary_logloss: 0.0898369\n",
            "[712]\ttraining's binary_logloss: 0.0897898\n",
            "[713]\ttraining's binary_logloss: 0.0897645\n",
            "[714]\ttraining's binary_logloss: 0.0897292\n",
            "[715]\ttraining's binary_logloss: 0.0896981\n",
            "[716]\ttraining's binary_logloss: 0.0896298\n",
            "[717]\ttraining's binary_logloss: 0.0895811\n",
            "[718]\ttraining's binary_logloss: 0.0895473\n",
            "[719]\ttraining's binary_logloss: 0.0894942\n",
            "[720]\ttraining's binary_logloss: 0.0894231\n",
            "[721]\ttraining's binary_logloss: 0.089375\n",
            "[722]\ttraining's binary_logloss: 0.0893489\n",
            "[723]\ttraining's binary_logloss: 0.0892947\n",
            "[724]\ttraining's binary_logloss: 0.0892653\n",
            "[725]\ttraining's binary_logloss: 0.0892336\n",
            "[726]\ttraining's binary_logloss: 0.0892026\n",
            "[727]\ttraining's binary_logloss: 0.0891411\n",
            "[728]\ttraining's binary_logloss: 0.0890879\n",
            "[729]\ttraining's binary_logloss: 0.0890669\n",
            "[730]\ttraining's binary_logloss: 0.0890064\n",
            "[731]\ttraining's binary_logloss: 0.0889528\n",
            "[732]\ttraining's binary_logloss: 0.0888999\n",
            "[733]\ttraining's binary_logloss: 0.0888419\n",
            "[734]\ttraining's binary_logloss: 0.088783\n",
            "[735]\ttraining's binary_logloss: 0.0887674\n",
            "[736]\ttraining's binary_logloss: 0.0887163\n",
            "[737]\ttraining's binary_logloss: 0.0886655\n",
            "[738]\ttraining's binary_logloss: 0.0886324\n",
            "[739]\ttraining's binary_logloss: 0.0885862\n",
            "[740]\ttraining's binary_logloss: 0.0885545\n",
            "[741]\ttraining's binary_logloss: 0.088511\n",
            "[742]\ttraining's binary_logloss: 0.0884953\n",
            "[743]\ttraining's binary_logloss: 0.0884739\n",
            "[744]\ttraining's binary_logloss: 0.0884397\n",
            "[745]\ttraining's binary_logloss: 0.0884127\n",
            "[746]\ttraining's binary_logloss: 0.0883759\n",
            "[747]\ttraining's binary_logloss: 0.0883399\n",
            "[748]\ttraining's binary_logloss: 0.0883213\n",
            "[749]\ttraining's binary_logloss: 0.0882924\n",
            "[750]\ttraining's binary_logloss: 0.0882764\n",
            "[751]\ttraining's binary_logloss: 0.0882279\n",
            "[752]\ttraining's binary_logloss: 0.088181\n",
            "[753]\ttraining's binary_logloss: 0.0881226\n",
            "[754]\ttraining's binary_logloss: 0.0881061\n",
            "[755]\ttraining's binary_logloss: 0.0880904\n",
            "[756]\ttraining's binary_logloss: 0.088076\n",
            "[757]\ttraining's binary_logloss: 0.0880237\n",
            "[758]\ttraining's binary_logloss: 0.0879743\n",
            "[759]\ttraining's binary_logloss: 0.0879385\n",
            "[760]\ttraining's binary_logloss: 0.0878983\n",
            "[761]\ttraining's binary_logloss: 0.0878541\n",
            "[762]\ttraining's binary_logloss: 0.0878171\n",
            "[763]\ttraining's binary_logloss: 0.0877651\n",
            "[764]\ttraining's binary_logloss: 0.0877179\n",
            "[765]\ttraining's binary_logloss: 0.0876923\n",
            "[766]\ttraining's binary_logloss: 0.0876485\n",
            "[767]\ttraining's binary_logloss: 0.0876239\n",
            "[768]\ttraining's binary_logloss: 0.087585\n",
            "[769]\ttraining's binary_logloss: 0.0875727\n",
            "[770]\ttraining's binary_logloss: 0.087538\n",
            "[771]\ttraining's binary_logloss: 0.0875045\n",
            "[772]\ttraining's binary_logloss: 0.0874637\n",
            "[773]\ttraining's binary_logloss: 0.0874356\n",
            "[774]\ttraining's binary_logloss: 0.087379\n",
            "[775]\ttraining's binary_logloss: 0.087329\n",
            "[776]\ttraining's binary_logloss: 0.0872859\n",
            "[777]\ttraining's binary_logloss: 0.0872466\n",
            "[778]\ttraining's binary_logloss: 0.0872017\n",
            "[779]\ttraining's binary_logloss: 0.0871648\n",
            "[780]\ttraining's binary_logloss: 0.0871198\n",
            "[781]\ttraining's binary_logloss: 0.0870935\n",
            "[782]\ttraining's binary_logloss: 0.0870694\n",
            "[783]\ttraining's binary_logloss: 0.0870202\n",
            "[784]\ttraining's binary_logloss: 0.086982\n",
            "[785]\ttraining's binary_logloss: 0.0869473\n",
            "[786]\ttraining's binary_logloss: 0.0869095\n",
            "[787]\ttraining's binary_logloss: 0.0868768\n",
            "[788]\ttraining's binary_logloss: 0.0868617\n",
            "[789]\ttraining's binary_logloss: 0.086825\n",
            "[790]\ttraining's binary_logloss: 0.0867957\n",
            "[791]\ttraining's binary_logloss: 0.0867815\n",
            "[792]\ttraining's binary_logloss: 0.0867694\n",
            "[793]\ttraining's binary_logloss: 0.0867559\n",
            "[794]\ttraining's binary_logloss: 0.0867405\n",
            "[795]\ttraining's binary_logloss: 0.0867165\n",
            "[796]\ttraining's binary_logloss: 0.0866767\n",
            "[797]\ttraining's binary_logloss: 0.0866417\n",
            "[798]\ttraining's binary_logloss: 0.0866139\n",
            "[799]\ttraining's binary_logloss: 0.0865738\n",
            "[800]\ttraining's binary_logloss: 0.0865371\n",
            "[801]\ttraining's binary_logloss: 0.0864847\n",
            "[802]\ttraining's binary_logloss: 0.0864611\n",
            "[803]\ttraining's binary_logloss: 0.0864234\n",
            "[804]\ttraining's binary_logloss: 0.0863807\n",
            "[805]\ttraining's binary_logloss: 0.0863621\n",
            "[806]\ttraining's binary_logloss: 0.0863266\n",
            "[807]\ttraining's binary_logloss: 0.0862991\n",
            "[808]\ttraining's binary_logloss: 0.0862484\n",
            "[809]\ttraining's binary_logloss: 0.0862344\n",
            "[810]\ttraining's binary_logloss: 0.0862008\n",
            "[811]\ttraining's binary_logloss: 0.0861534\n",
            "[812]\ttraining's binary_logloss: 0.0861042\n",
            "[813]\ttraining's binary_logloss: 0.0860783\n",
            "[814]\ttraining's binary_logloss: 0.0860559\n",
            "[815]\ttraining's binary_logloss: 0.0860089\n",
            "[816]\ttraining's binary_logloss: 0.0859911\n",
            "[817]\ttraining's binary_logloss: 0.0859771\n",
            "[818]\ttraining's binary_logloss: 0.0859449\n",
            "[819]\ttraining's binary_logloss: 0.0859267\n",
            "[820]\ttraining's binary_logloss: 0.0859157\n",
            "[821]\ttraining's binary_logloss: 0.0858867\n",
            "[822]\ttraining's binary_logloss: 0.085853\n",
            "[823]\ttraining's binary_logloss: 0.0858203\n",
            "[824]\ttraining's binary_logloss: 0.0857855\n",
            "[825]\ttraining's binary_logloss: 0.085747\n",
            "[826]\ttraining's binary_logloss: 0.0857168\n",
            "[827]\ttraining's binary_logloss: 0.0856777\n",
            "[828]\ttraining's binary_logloss: 0.085667\n",
            "[829]\ttraining's binary_logloss: 0.0856142\n",
            "[830]\ttraining's binary_logloss: 0.0855732\n",
            "[831]\ttraining's binary_logloss: 0.0855564\n",
            "[832]\ttraining's binary_logloss: 0.0855327\n",
            "[833]\ttraining's binary_logloss: 0.0854998\n",
            "[834]\ttraining's binary_logloss: 0.0854851\n",
            "[835]\ttraining's binary_logloss: 0.0854617\n",
            "[836]\ttraining's binary_logloss: 0.0854267\n",
            "[837]\ttraining's binary_logloss: 0.0854112\n",
            "[838]\ttraining's binary_logloss: 0.0853944\n",
            "[839]\ttraining's binary_logloss: 0.0853817\n",
            "[840]\ttraining's binary_logloss: 0.085368\n",
            "[841]\ttraining's binary_logloss: 0.0853366\n",
            "[842]\ttraining's binary_logloss: 0.0852966\n",
            "[843]\ttraining's binary_logloss: 0.0852807\n",
            "[844]\ttraining's binary_logloss: 0.0852328\n",
            "[845]\ttraining's binary_logloss: 0.0852034\n",
            "[846]\ttraining's binary_logloss: 0.0851664\n",
            "[847]\ttraining's binary_logloss: 0.0851486\n",
            "[848]\ttraining's binary_logloss: 0.0851273\n",
            "[849]\ttraining's binary_logloss: 0.0850982\n",
            "[850]\ttraining's binary_logloss: 0.0850857\n",
            "[851]\ttraining's binary_logloss: 0.0850333\n",
            "[852]\ttraining's binary_logloss: 0.0850218\n",
            "[853]\ttraining's binary_logloss: 0.0850123\n",
            "[854]\ttraining's binary_logloss: 0.0849983\n",
            "[855]\ttraining's binary_logloss: 0.0849431\n",
            "[856]\ttraining's binary_logloss: 0.0848959\n",
            "[857]\ttraining's binary_logloss: 0.0848606\n",
            "[858]\ttraining's binary_logloss: 0.0848206\n",
            "[859]\ttraining's binary_logloss: 0.0847762\n",
            "[860]\ttraining's binary_logloss: 0.0847398\n",
            "[861]\ttraining's binary_logloss: 0.0846926\n",
            "[862]\ttraining's binary_logloss: 0.0846617\n",
            "[863]\ttraining's binary_logloss: 0.0846135\n",
            "[864]\ttraining's binary_logloss: 0.0845717\n",
            "[865]\ttraining's binary_logloss: 0.0845483\n",
            "[866]\ttraining's binary_logloss: 0.084537\n",
            "[867]\ttraining's binary_logloss: 0.0845199\n",
            "[868]\ttraining's binary_logloss: 0.0844792\n",
            "[869]\ttraining's binary_logloss: 0.084466\n",
            "[870]\ttraining's binary_logloss: 0.0844537\n",
            "[871]\ttraining's binary_logloss: 0.0844134\n",
            "[872]\ttraining's binary_logloss: 0.0843791\n",
            "[873]\ttraining's binary_logloss: 0.0843472\n",
            "[874]\ttraining's binary_logloss: 0.0843326\n",
            "[875]\ttraining's binary_logloss: 0.084296\n",
            "[876]\ttraining's binary_logloss: 0.0842835\n",
            "[877]\ttraining's binary_logloss: 0.0842623\n",
            "[878]\ttraining's binary_logloss: 0.0842513\n",
            "[879]\ttraining's binary_logloss: 0.0842101\n",
            "[880]\ttraining's binary_logloss: 0.0841605\n",
            "[881]\ttraining's binary_logloss: 0.0841089\n",
            "[882]\ttraining's binary_logloss: 0.0840701\n",
            "[883]\ttraining's binary_logloss: 0.0840419\n",
            "[884]\ttraining's binary_logloss: 0.0839974\n",
            "[885]\ttraining's binary_logloss: 0.08394\n",
            "[886]\ttraining's binary_logloss: 0.0838947\n",
            "[887]\ttraining's binary_logloss: 0.0838594\n",
            "[888]\ttraining's binary_logloss: 0.0838451\n",
            "[889]\ttraining's binary_logloss: 0.0838314\n",
            "[890]\ttraining's binary_logloss: 0.08381\n",
            "[891]\ttraining's binary_logloss: 0.0837993\n",
            "[892]\ttraining's binary_logloss: 0.0837758\n",
            "[893]\ttraining's binary_logloss: 0.0837485\n",
            "[894]\ttraining's binary_logloss: 0.0837154\n",
            "[895]\ttraining's binary_logloss: 0.083682\n",
            "[896]\ttraining's binary_logloss: 0.0836699\n",
            "[897]\ttraining's binary_logloss: 0.0836359\n",
            "[898]\ttraining's binary_logloss: 0.083605\n",
            "[899]\ttraining's binary_logloss: 0.0835635\n",
            "[900]\ttraining's binary_logloss: 0.0835324\n",
            "[901]\ttraining's binary_logloss: 0.0835171\n",
            "[902]\ttraining's binary_logloss: 0.083469\n",
            "[903]\ttraining's binary_logloss: 0.0834364\n",
            "[904]\ttraining's binary_logloss: 0.0833919\n",
            "[905]\ttraining's binary_logloss: 0.0833628\n",
            "[906]\ttraining's binary_logloss: 0.0833092\n",
            "[907]\ttraining's binary_logloss: 0.0832978\n",
            "[908]\ttraining's binary_logloss: 0.0832514\n",
            "[909]\ttraining's binary_logloss: 0.0832086\n",
            "[910]\ttraining's binary_logloss: 0.0831872\n",
            "[911]\ttraining's binary_logloss: 0.0831785\n",
            "[912]\ttraining's binary_logloss: 0.0831446\n",
            "[913]\ttraining's binary_logloss: 0.0831095\n",
            "[914]\ttraining's binary_logloss: 0.083077\n",
            "[915]\ttraining's binary_logloss: 0.0830396\n",
            "[916]\ttraining's binary_logloss: 0.0830289\n",
            "[917]\ttraining's binary_logloss: 0.0829993\n",
            "[918]\ttraining's binary_logloss: 0.0829529\n",
            "[919]\ttraining's binary_logloss: 0.0829407\n",
            "[920]\ttraining's binary_logloss: 0.0829198\n",
            "[921]\ttraining's binary_logloss: 0.0828922\n",
            "[922]\ttraining's binary_logloss: 0.082884\n",
            "[923]\ttraining's binary_logloss: 0.0828459\n",
            "[924]\ttraining's binary_logloss: 0.0828201\n",
            "[925]\ttraining's binary_logloss: 0.0827797\n",
            "[926]\ttraining's binary_logloss: 0.0827519\n",
            "[927]\ttraining's binary_logloss: 0.0827431\n",
            "[928]\ttraining's binary_logloss: 0.0827163\n",
            "[929]\ttraining's binary_logloss: 0.0826887\n",
            "[930]\ttraining's binary_logloss: 0.0826697\n",
            "[931]\ttraining's binary_logloss: 0.0826351\n",
            "[932]\ttraining's binary_logloss: 0.0825993\n",
            "[933]\ttraining's binary_logloss: 0.0825615\n",
            "[934]\ttraining's binary_logloss: 0.0825227\n",
            "[935]\ttraining's binary_logloss: 0.0824821\n",
            "[936]\ttraining's binary_logloss: 0.0824548\n",
            "[937]\ttraining's binary_logloss: 0.0824168\n",
            "[938]\ttraining's binary_logloss: 0.0824076\n",
            "[939]\ttraining's binary_logloss: 0.0823887\n",
            "[940]\ttraining's binary_logloss: 0.0823555\n",
            "[941]\ttraining's binary_logloss: 0.0823469\n",
            "[942]\ttraining's binary_logloss: 0.0823146\n",
            "[943]\ttraining's binary_logloss: 0.0823047\n",
            "[944]\ttraining's binary_logloss: 0.082288\n",
            "[945]\ttraining's binary_logloss: 0.0822466\n",
            "[946]\ttraining's binary_logloss: 0.0822095\n",
            "[947]\ttraining's binary_logloss: 0.0821836\n",
            "[948]\ttraining's binary_logloss: 0.082175\n",
            "[949]\ttraining's binary_logloss: 0.0821474\n",
            "[950]\ttraining's binary_logloss: 0.0821377\n",
            "[951]\ttraining's binary_logloss: 0.0820684\n",
            "[952]\ttraining's binary_logloss: 0.0820362\n",
            "[953]\ttraining's binary_logloss: 0.082002\n",
            "[954]\ttraining's binary_logloss: 0.0819703\n",
            "[955]\ttraining's binary_logloss: 0.0819364\n",
            "[956]\ttraining's binary_logloss: 0.0819001\n",
            "[957]\ttraining's binary_logloss: 0.0818754\n",
            "[958]\ttraining's binary_logloss: 0.0818419\n",
            "[959]\ttraining's binary_logloss: 0.0818326\n",
            "[960]\ttraining's binary_logloss: 0.0818155\n",
            "[961]\ttraining's binary_logloss: 0.0817887\n",
            "[962]\ttraining's binary_logloss: 0.0817539\n",
            "[963]\ttraining's binary_logloss: 0.0817093\n",
            "[964]\ttraining's binary_logloss: 0.0816673\n",
            "[965]\ttraining's binary_logloss: 0.0816349\n",
            "[966]\ttraining's binary_logloss: 0.0815893\n",
            "[967]\ttraining's binary_logloss: 0.0815702\n",
            "[968]\ttraining's binary_logloss: 0.0815494\n",
            "[969]\ttraining's binary_logloss: 0.0815196\n",
            "[970]\ttraining's binary_logloss: 0.0814799\n",
            "[971]\ttraining's binary_logloss: 0.0814509\n",
            "[972]\ttraining's binary_logloss: 0.081416\n",
            "[973]\ttraining's binary_logloss: 0.0813808\n",
            "[974]\ttraining's binary_logloss: 0.0813204\n",
            "[975]\ttraining's binary_logloss: 0.0812876\n",
            "[976]\ttraining's binary_logloss: 0.0812777\n",
            "[977]\ttraining's binary_logloss: 0.0812671\n",
            "[978]\ttraining's binary_logloss: 0.0812356\n",
            "[979]\ttraining's binary_logloss: 0.0812123\n",
            "[980]\ttraining's binary_logloss: 0.0811703\n",
            "[981]\ttraining's binary_logloss: 0.0811459\n",
            "[982]\ttraining's binary_logloss: 0.0811154\n",
            "[983]\ttraining's binary_logloss: 0.081103\n",
            "[984]\ttraining's binary_logloss: 0.0810932\n",
            "[985]\ttraining's binary_logloss: 0.0810849\n",
            "[986]\ttraining's binary_logloss: 0.0810542\n",
            "[987]\ttraining's binary_logloss: 0.0810258\n",
            "[988]\ttraining's binary_logloss: 0.0809883\n",
            "[989]\ttraining's binary_logloss: 0.080975\n",
            "[990]\ttraining's binary_logloss: 0.0809339\n",
            "[991]\ttraining's binary_logloss: 0.0809164\n",
            "[992]\ttraining's binary_logloss: 0.0808771\n",
            "[993]\ttraining's binary_logloss: 0.0808357\n",
            "[994]\ttraining's binary_logloss: 0.0808004\n",
            "[995]\ttraining's binary_logloss: 0.0807912\n",
            "[996]\ttraining's binary_logloss: 0.0807591\n",
            "[997]\ttraining's binary_logloss: 0.0807439\n",
            "[998]\ttraining's binary_logloss: 0.0807322\n",
            "[999]\ttraining's binary_logloss: 0.0806971\n",
            "[1000]\ttraining's binary_logloss: 0.08067\n",
            "26.640957832336426\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}